{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutions and Convolutional Neural Networks</h1>\n",
    "<img src=\"https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=\"750\" align=\"center\">\n",
    "The 2D Image convolution is a fairly simple operation that leads to powerful and somewhat surprising results! In this notebook we'll look at performing convolutions with a hand-crafted kernal and then look at how we can learn the parameters of a kernal to perform some task!\n",
    "\n",
    "Have a look at this interactive convolution visualiser\n",
    "[Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a test image to experiment with using the Python Imaging Library (PIL).<br>\n",
    "Note: PIL images are thenselves objects and the image can be displayed just by printing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img1 = Image.open(\"Plane1.png\").convert('RGB')\n",
    "test_img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a Pytorch \"transform\" using the torchvision library \n",
    "#This particular transform simply takes a PIL image and converts it to a tensor\n",
    "transform = T.ToTensor()\n",
    "test_img1 = transform(test_img1)\n",
    "print(\"Image Shape: \", test_img1.shape)\n",
    "#NOTE:Many torchvision functions with only work on PIL images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hand Crafted Convolution Kernels</h2>\n",
    "\n",
    "<h3>Sobel Edge Detector</h3>\n",
    "The <a href=\"https://en.wikipedia.org/wiki/Sobel_operator\">Sobel Edge detector</a> is a famous and simple convolutional kernal filter that will \"extract\" the edges of an image and was/is extensively used as a part of many algorithms. Here we will create a Sobel Filter and use it on our test image.<br> By looking at the filter values can you tell how it works?<br><br>\n",
    "Convolution with a Sobel Kernel (left) and the features extracted by a Sobel edge detector (right)<br>\n",
    "<img src=\"https://miro.medium.com/max/1356/1*-OM6jQTMNACDX2vAh_lvMQ.png\" width=\"480\" align=\"left\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/17/Bikesgraysobel.jpg\" width=\"480\" align=\"right\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we must create a filter that will extract edges in the X direction\n",
    "Gx = torch.FloatTensor([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]]).unsqueeze(0)\n",
    "Gx = torch.repeat_interleave(Gx, 3, 0).unsqueeze(0)\n",
    "print(\"Kernel Shape: \", Gx.shape)\n",
    "print(Gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we must create a filter that will extract edges in the Y direction\n",
    "Gy = torch.FloatTensor([[1, 2, 1],[0, 0, 0],[-1, -2, -1]]).unsqueeze(0)\n",
    "Gy = torch.repeat_interleave(Gy, 3, 0).unsqueeze(0)\n",
    "print(\"Kernel Shape: \", Gy.shape)\n",
    "print(Gy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutions in Pytorch </h3>\n",
    "For the function <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html\">Conv2d</a> (which performs the convolution operation) the convolutional kernel must be of the shape  <br>\n",
    "<b>[out channels, in channels, kernel height, kernel width]</b> <br>\n",
    "The input image must have the shape <br>\n",
    "<b>[minibatch size, in channels, image height, image width]</b> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolve the image with the X direction kernel\n",
    "conv_out1 = F.conv2d(test_img1.unsqueeze(0), Gx, bias=None, stride=1)\n",
    "plt.imshow(torch.squeeze(conv_out1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolve the image with the Y direction kernel\n",
    "conv_out2 = F.conv2d(test_img1.unsqueeze(0), Gy, bias=None, stride=1)\n",
    "plt.imshow(torch.squeeze(conv_out2), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two resulting images together by finding the magnitude\n",
    "conv_out = (conv_out1.pow(2) + conv_out2.pow(2)).sqrt()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(torch.squeeze(conv_out), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spot the Difference!</h2>\n",
    "Let's play a game of spot the difference and use convolutional filters to help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load the second test image\n",
    "test_img2 = Image.open(\"Plane2.png\").convert('RGB')\n",
    "test_img2 = transform(test_img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's plot them side-by-side\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize = (20,20))\n",
    "ax1.imshow(test_img1.numpy().transpose((1,2,0)))\n",
    "ax2.imshow(test_img2.numpy().transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Difference Image?</h2>\n",
    "Hmmmmmmmm they look pretty similar... how about we just subtract one from another??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.imshow(test_img2.numpy().transpose((1,2,0)) - test_img1.numpy().transpose((1,2,0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmmm, looks like any difference between them is very small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Sobel Edges? </h2>\n",
    "Maybe our friend the Sobel edge detector will help us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_out1Y = F.conv2d(test_img1.unsqueeze(0), Gy, bias=None, stride=1)\n",
    "img_out1X = F.conv2d(test_img1.unsqueeze(0), Gx, bias=None, stride=1)\n",
    "img_out1  = (img_out1Y.pow(2) + img_out1X.pow(2)).sqrt()\n",
    "\n",
    "img_out2Y = F.conv2d(test_img2.unsqueeze(0), Gy, bias=None, stride=1)\n",
    "img_out2X = F.conv2d(test_img2.unsqueeze(0), Gx, bias=None, stride=1)\n",
    "img_out2  = (img_out2Y.pow(2) + img_out2X.pow(2)).sqrt()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize = (20,10))\n",
    "ax1.imshow(torch.squeeze(img_out1.detach()), cmap='gray')\n",
    "ax2.imshow(torch.squeeze(img_out2.detach()), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! They still look the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train a Convolution Kernel! </h2>\n",
    "So how do you look for something when you don't know what you are looking for? Often in computer vision we know WHAT we want, but we aren't sure how to get it! For example if you want to create an algorithm to classify cats, you might start by creating a list of all the uniquely cat-like things about an image of a cat, but then how do you go about extracting them from an image?? What if there's something you forgot? <br>\n",
    "So in comes machine learning, we specify an objective (a cost function or a loss) for some learning model with the hope that by minimizing that loss our model will do what we want! (as you may have learnt by now there's a LOT more to it then that)<br>\n",
    "So what objective can we come up with to see the difference between these two images?\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-b662a8fc3be57f76c708c171fcf29960\" width=\"480\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html\n",
    "#torch.nn.Conv2d)\n",
    "\n",
    "#Lets create a learnable 2D convolutional layer\n",
    "#in_channels  - the number of input channels\n",
    "#out_channels - the number of output channels - also the number of kernels in a layer\n",
    "#kernel_size  - the height and width of our kernel - can specify with a tuple for non-square kernels\n",
    "#stride       - the number of pixels the kernel will \"step\"\n",
    "#bias         - same as a linear layer, 1 bias term per output channel\n",
    "\n",
    "conv_kernel = nn.Conv2d(in_channels = 3, out_channels = 1, kernel_size = 3, stride = 1, bias = False)\n",
    "\n",
    "#Define an optimizer\n",
    "optimizer = optim.SGD(conv_kernel.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the kernel's shape\n",
    "conv_kernel.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll pass both images through the randomly initialised convolutional layers\n",
    "img_out1 = conv_kernel(test_img1.unsqueeze(0))\n",
    "img_out2 = conv_kernel(test_img2.unsqueeze(0))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize = (20,10))\n",
    "ax1.imshow(torch.squeeze(img_out1.detach()), cmap='gray')\n",
    "ax2.imshow(torch.squeeze(img_out2.detach()), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training our kernel!</b><br>\n",
    "Have you come up with a loss function to use yet? Perform GD with it and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger =[]\n",
    "for _ in range(100):\n",
    "    img_out1 = conv_kernel(test_img1.unsqueeze(0))\n",
    "    img_out2 = conv_kernel(test_img2.unsqueeze(0))\n",
    "\n",
    "    loss = #######TO DO##########\n",
    "    logger.append(loss.item())\n",
    "    conv_kernel.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    clear_output(True)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code you can see what we did! After passing each image through the convolutional layer we optimise the parameters so the resulting images are as far apart as possible. That way the kernel will exaggerate any differences present in the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize = (20,10))\n",
    "ax1.imshow(torch.squeeze(img_out1.detach()), cmap='gray')\n",
    "ax2.imshow(torch.squeeze(img_out2.detach()), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv_kernel.weight.data.round())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
