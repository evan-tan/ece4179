{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_JzTtzutxjU"
   },
   "source": [
    "<h1> Lung segmentation of Chest X-Rays </h1>\n",
    "Image segmentation is an interesting and widely used application for Neural Networks.<br>\n",
    "We can think of segmentation as a \"per-pixel\" image classification where the input is not a vector of class activations for the whole image, but class activations for every pixel! We train the segmentation network very similarly to a simple classification and can even use the same loss function. The difference comes with the structure of our network. As was hinted the output of a segmentation network needs to be an image with as many channels as there are classes. Therefore a simple downsampling network will not work!<br>\n",
    "Instead something like an Autoencoder network must be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Autoencoders</h3>\n",
    "Autoencoders are a fairly straightforward network structure, characterised by a \"bottleneck\" where the input is \"compressed\" before being upsampled again. This network can be used to create compressed representations of images by training the model to reconstruct the input on the output. It could also be used for our segmentation problem! However in segmentation, we don't really want our network to compress our image, we want it to do some \"work\" and then give us a segmented version of the input!\n",
    "<img src=\"https://miro.medium.com/max/3148/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "[Autoencoders](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOQv-PMmtxjW"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "data_path =\"Chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device to GPU_indx if GPU is avaliable\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create our Dataset</h2>\n",
    "Along with this notebook came a folder for our data which is split into folders, one containing the input images and the other the segmentation mask (label). Along with the data is a CSV file which specifies which data should be used for training and testing.The following dataset reads the test/train split CSV and saves the filenames of either the training or testing split.<br>\n",
    "There are many ways to construct such a dataset class as well as the format of your dataset folder structure, this is just one common way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6y7Gfkntxjc"
   },
   "outputs": [],
   "source": [
    "#Dataset class for our lung data (used by the data loader)\n",
    "class LungDataset(Dataset):\n",
    "    def __init__(self, root_data_dir, training = True):\n",
    "        \n",
    "        #Load all the filenames and their train/test indexs\n",
    "        #For every filename there is a 1 or a 0 indicating that it belongs to the \n",
    "        #training set or test set\n",
    "        train_test = np.loadtxt(root_data_dir + \"/Train_Test_split.csv\", dtype=\"<U8\")\n",
    "        \n",
    "        #Convert the indexs from str to int and compare this to \"training\"\n",
    "        #1 = True (Training)\n",
    "        #0 = False (Testing)\n",
    "        #Then use the boolean array to index the filenames\n",
    "        self.filenames = train_test[(train_test[:, 1].astype(int) == training), 0]\n",
    "        \n",
    "        #ToTensor object for converting image to tensor\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "        \n",
    "        self.root_data_dir = root_data_dir\n",
    "\n",
    "    #Returns a single image and label pair\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #Read image and labels\n",
    "        image = Image.open(self.root_data_dir + '/images/' + self.filenames[index])\n",
    "        label = Image.open(self.root_data_dir + '/labels/' + self.filenames[index])\n",
    "\n",
    "        #Make image in range (-1,1)\n",
    "        image = self.to_tensor(image)\n",
    "        image = (image-0.5)/0.5\n",
    "        image = image[0:1,:,:] #Images are grayscale, only need one channel\n",
    "        \n",
    "        #Cross entropy loss needs labels as LongTensor type\n",
    "        label = self.to_tensor(label).type(torch.LongTensor).squeeze(0)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH4eidyjtxjZ"
   },
   "source": [
    "<h3>Create a dataset and dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TlOmD35txje"
   },
   "outputs": [],
   "source": [
    "# Create train and test dataset\n",
    "dataset_train = LungDataset(data_path)\n",
    "dataset_test = LungDataset(data_path,False)\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zysqIam9txjh"
   },
   "source": [
    "<h3> Perform Sanity Check </h3>\n",
    "It is prudent to perform sanity check of the data correspondance. It become a routine check-up after a while but it is very crucial to check if we had made a mistake in loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3536,
     "status": "ok",
     "timestamp": 1568333935611,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "KCpRreK7txjh",
    "outputId": "3001c45a-dbf4-405f-e53b-fb05e4ce9144"
   },
   "outputs": [],
   "source": [
    "#create a dataloader itterable object\n",
    "dataloader_it = iter(data_loader_train)\n",
    "#sample from the itterable object\n",
    "image, label = next(dataloader_it)\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "img_out = torchvision.utils.make_grid(((image+1)/2)[0:4], 4)\n",
    "lbl_out = torchvision.utils.make_grid(label[0:4].unsqueeze(1), 4).float()\n",
    "\n",
    "out = torch.cat((img_out, lbl_out), 1)\n",
    "\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The U-Net</h2>\n",
    "The U-Net was developed specifically for image segmentation, the intuition being that the \"autoencoder-like\" structure will extract class information from the input image and the skip connections allow image \"structure\" information (contained in the feature maps) to jump the bottle-neck. This means that the network does not have to \"learn\" how to extract and compress the structure of the image leading to sharper edges and higher quality results.\n",
    "<img src=\"https://miro.medium.com/max/1200/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "[U-Net](https://towardsdatascience.com/u-net-b229b32b4a71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transpose Convolution</h3>\n",
    "The U-Net model also introduces a new layer-type the \"Transpose convolution\" (sometimes called \"Deconvolution\")<br>\n",
    "The transpose convolution is a \"learnable upsampling\" method and is essentially the opposite of a convolution! We take a single feature (pixel) in our feature map and replicate it and multiply by a kernel, any overlapping sections are added together. The easiest way to understand them is with the following animation (where the blue square is the input and green is the output).\n",
    "<img src=\"https://miro.medium.com/max/986/1*yoQ62ckovnGYV2vSIq9q4g.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "[Transpose Convolution](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8)\n",
    "\n",
    "[Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycO6FReFtxjk"
   },
   "source": [
    "<h3> Define the network - U-Net</h3>\n",
    "We will construct a small verson of the Unet model in two ways: Unet1 and Unet2.<br>\n",
    "Both of these two network structure are identical. The method we used for Unet1 is easier to visualise and understand, but Unet2 is modular, which allows easier adding/removing/modification of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Basic model construction using Sequential blocks</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIrWZN6maSAR"
   },
   "outputs": [],
   "source": [
    "# Unet1\n",
    "class Unet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(Unet1, self).__init__()\n",
    "        \n",
    "        #Unless stated otherwise use the default values for each layer etc\n",
    "        \n",
    "        #We're going to be using Sequential blocks to build up the network\n",
    "        #Remember for nn.Sequential we pass the class constructors (ie nn.Conv2d()) not the instance\n",
    "        \n",
    "        # Define the first double conv layers, it contains\n",
    "        # 1. conv 32-channels out, 3x3 kernal and padding of 1\n",
    "        # The purpose of padding is to retain the output shape of each channel same as input.\n",
    "        # 2. add a nn.ReLU() to activate the conv layer\n",
    "        # 3. conv 32-channels out, 3x3 kernal and padding of 1\n",
    "        # 4. another relu\n",
    "        self.doubleconv1 = nn.Sequential#####TO DO######\n",
    "                                   \n",
    "        \n",
    "        # The second block contains a maxpooling and two conv layers followed by relu\n",
    "        # 1. maxpooling to halve the image size with a kernel size 2x2 and stride 2\n",
    "        # 2. conv 64-channels out, 3x3 kernal and padding of 1\n",
    "        # 3. relu\n",
    "        # 4. conv 64-channels out, 3x3 kernal and padding of 1\n",
    "        # 5. another relu\n",
    "        self.down2 = nn.Sequential#####TO DO######\n",
    "        \n",
    "        # The third block:\n",
    "        # 1. maxpooling to halve the image size\n",
    "        # 2. conv 128-channels out, 3x3 kernal and padding of 1\n",
    "        # 3. relu\n",
    "        # 4. conv 128-channels out, 3x3 kernal and padding of 1\n",
    "        # 5. another relu\n",
    "        self.down3 =  nn.Sequential#####TO DO######\n",
    "\n",
    "        \n",
    "        # Now as we reach the bottleneck of our network we want upsample to double the size of each channel\n",
    "        # use nn.ConvTranspose2d with 64-channels out, 2x2 kernal and stride of 2\n",
    "        self.up4 = #####TO DO######\n",
    "        \n",
    "        # 1. conv 128-channels in, 64-channels out, 3x3 kernal and padding of 1\n",
    "        # the number of channels in is 128 is because we concatenate the output of self.up4 and the output of self.down2\n",
    "        # 2. relu\n",
    "        # 3. conv 64-channels out, 3x3 kernal and padding of 1\n",
    "        # 4. another relu\n",
    "        self.doubleconv5 = nn.Sequential#####TO DO######\n",
    "            \n",
    "        # use nn.ConvTranspose2d with 32-channels out, 2x2 kernal and stride of 2\n",
    "        self.up6 = #####TO DO######\n",
    "        \n",
    "        # 1. conv 64-channels in, 32-channels out, 3x3 kernal and padding of 1\n",
    "        # 2. relu\n",
    "        # 3. conv 32-channels out, 3x3 kernal and padding of 1\n",
    "        # 4. another relu\n",
    "        # 5. conv 2-channels out, 1x1 kernal and no padding\n",
    "        self.doubleconv7 = nn.Sequential#####TO DO######\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the input to the network\n",
    "        # x is 1 x 64 x 64\n",
    "        x1 = self.doubleconv1(x)\n",
    "        # x1 = 32 x 64 x 64\n",
    "        x2 = self.down2(x1)\n",
    "        # x2 = 64 x 32 x 32\n",
    "        x3 = self.down3(x2)\n",
    "        # x3 = 128 x 16 x 16\n",
    "        x4 = self.up4(x3)\n",
    "        # x4 = 64 x 32 x 32\n",
    "        # torch.cat([x4,x2]) = 128 x 32 x 32 \n",
    "        #Perform the first skip connection\n",
    "        x5 = self.doubleconv5(torch.cat([x4,x2],dim=1))\n",
    "        # x5 = 64 x 32 x 32\n",
    "        x6 = self.up6(x5)\n",
    "        # x6 = 32 x 64 x 64\n",
    "        # torch.cat([x6,x1]) = 64 x 64 x 64\n",
    "        #Perform the second skip connection\n",
    "        x7 = self.doubleconv7(torch.cat([x6,x1],dim=1))\n",
    "        # x7 = 2 x 64 x 64\n",
    "    \n",
    "        return x7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Advanced model construction using nn.module blocks</h4>\n",
    "By creating our model using seperate nn.module classes it's easy to expand and modify our network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49zhhI_Ltxjl"
   },
   "outputs": [],
   "source": [
    "#Unet2\n",
    "#Unless stated otherwise use the default values for each layer etc\n",
    "\n",
    "class Unetdown(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, first_layer = False):\n",
    "        super(Unetdown, self).__init__()\n",
    "        \n",
    "        model = []\n",
    "        if not first_layer:\n",
    "            model += [#maxpooling to halve the image size]\n",
    "        \n",
    "        model += [#conv2d input_nc, output_nc, kernel 3x3, padding 1\n",
    "                  #relu\n",
    "                  #conv2d output_nc, output_nc, kernel 3x3, padding 1\n",
    "                  #relu\n",
    "                 ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "\n",
    "class Unetup(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, last_layer = False):\n",
    "        super(Unetup, self).__init__()\n",
    "\n",
    "        self.up = #transpose convolution kernel 2x2 stride 2 \n",
    "\n",
    "        model = []\n",
    "        model += [#conv2d input_nc, output_nc, kernel 3x3, padding 1\n",
    "                  #relu\n",
    "                  #conv2d output_nc, output_nc, kernel 3x3, padding 1\n",
    "                  #relu\n",
    "                 ]\n",
    "        \n",
    "        if last_layer:\n",
    "            model += [#conv2d output_nc, out channels = 2, kernel 2x2]\n",
    "          \n",
    "        self.model = nn.Sequential(*model)\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        out = self.model(torch.cat([x1,x2],dim=1))\n",
    "        \n",
    "        return out\n",
    "            \n",
    "         \n",
    "class Unet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet2, self).__init__()\n",
    "        \n",
    "        self.down1 = ## Unetdown, channel in = 1, channel out = 32, first layer = True\n",
    "        self.down2 = ## Unetdown, channel in = 32, channel out = 64, first layer = False\n",
    "        self.down3 = ## Unetdown, channel in = 64, channel out = 128, first layer = True \n",
    "        self.up4 = ## Unetup, channel in = 128, channel out = 64, last layer = False \n",
    "        self.up5 = ## Unetup, channel in = 64, channel out = 32, last layer = True \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.up4(x3, x2)\n",
    "        x5 = self.up5(x4, x1)\n",
    "        \n",
    "        return x5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create our Model and optimizer </h3>\n",
    "Try out the two different model types, they should give the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7185,
     "status": "ok",
     "timestamp": 1568333950426,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "_Q6wuqf1txjn",
    "outputId": "2d270bda-da14-460b-9b0c-03fee37d8af1"
   },
   "outputs": [],
   "source": [
    "#Create UNet model - output is size (batch_size x 2 x H x W)\n",
    "#The two channels correspond to the two classes: not lung (class 0) and lung (class 1).\n",
    "\n",
    "#Create a model and cast it to device\n",
    "model = ##TO DO##\n",
    "print(model)\n",
    "\n",
    "#Use an Adam optimiser to update the weights of the model\n",
    "optimiser = ##TO DO##\n",
    "\n",
    "#Cross entropy - softmax over the two classes and negative log liklihood loss\n",
    "loss_fn = ##TO DO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6I6gyaBtxjq"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43073,
     "status": "ok",
     "timestamp": 1568333987155,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "HR2Q73Ustxjr",
    "outputId": "199b5227-4868-4490-f308-56a213603b87"
   },
   "outputs": [],
   "source": [
    "#Set maximum epochs and create empty lists to store losses\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_test = 0.0\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    #For each training batch...\n",
    "    for i, (image, label) in enumerate(data_loader_train):   \n",
    "\n",
    "        #forward pass through model \n",
    "        #make sure you cast the input to device\n",
    "        outputs = ##TO DO##\n",
    "        \n",
    "        #Compute cross entropy loss\n",
    "        #make sure you cast the label to device\n",
    "        loss = ##TO DO##\n",
    "        \n",
    "        #accumulate the running loss\n",
    "        running_loss_train += ##TO DO##\n",
    "\n",
    "        #Gradients are accumulated, so they should be zeroed before calling backwards\n",
    "        ##TO DO##\n",
    "        \n",
    "        #Backward pass through model and update the model weights\n",
    "        ##TO DO##\n",
    "        ##TO DO##\n",
    "        \n",
    "    running_loss_train /= len(data_loader_train)\n",
    "    Train_loss.append(running_loss_train)\n",
    "    \n",
    "    #Compute validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (image, label) in enumerate(data_loader_test):   \n",
    "\n",
    "            #forward pass through model \n",
    "            #make sure you cast the input to device\n",
    "            outputs = ##TO DO##\n",
    "            \n",
    "            #Compute cross entropy loss\n",
    "            #make sure you cast the label to device\n",
    "            loss = ##TO DO##\n",
    "            \n",
    "            #accumulate the running loss\n",
    "            running_loss_test += ##TO DO##\n",
    "    \n",
    "    running_loss_test /= len(data_loader_test)\n",
    "    Test_loss.append(running_loss_test)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    clear_output(True)\n",
    "    print('[Epoch {0:02d}] Train Loss: {1:.4f}, Val Loss: {2:.4f}, Time: {3:.4f}s'.format(epoch, running_loss_train, running_loss_test,end_time - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fheMmjnctxjt"
   },
   "source": [
    "## Plot the metric and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42194,
     "status": "ok",
     "timestamp": 1568333987155,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "VpFsKWPYtxju",
    "outputId": "4ab564c3-0620-4025-fd79-65435f32faf0"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(Train_loss, '-', label = 'Training Loss')\n",
    "plt.plot(Test_loss, '-', label = 'Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jq8bT-_Utxjw"
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42833,
     "status": "ok",
     "timestamp": 1568333988913,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "D0vPiL3ntxjx",
    "outputId": "16832fa3-ba62-4b03-92d2-6404d4238a04"
   },
   "outputs": [],
   "source": [
    "data_loader_iter = iter(data_loader_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        image, label = next(data_loader_iter)\n",
    "        \n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(image[0,0,:,:], cmap='gray')\n",
    "        plt.xlabel(\"Base Image\")\n",
    "        \n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output,dim=1,keepdim=True)\n",
    "        \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(label[0,:,:], cmap='gray')\n",
    "        plt.xlabel(\"Ground Truth\")\n",
    "        \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(pred.cpu().numpy()[0,0,:,:], cmap='gray')\n",
    "        plt.xlabel(\"Prediction\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chest_XRay_Segmentation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
