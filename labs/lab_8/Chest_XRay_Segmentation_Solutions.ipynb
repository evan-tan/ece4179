{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_JzTtzutxjU"
   },
   "source": [
    "<h1> Lung segmentation of Chest X-Rays </h1>\n",
    "Image segmentation is an interesting and widely used application for Neural Networks.<br>\n",
    "We can think of segmentation as a \"per-pixel\" image classification where the input is not a vector of class activations for the whole image, but class activations for every pixel! We train the segmentation network very similarly to a simple classification and can even use the same loss function. The difference comes with the structure of our network. As was hinted the output of a segmentation network needs to be an image with as many channels as there are classes. Therefore a simple downsampling network will not work!<br>\n",
    "Instead something like an Autoencoder network must be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Autoencoders</h3>\n",
    "Autoencoders are a fairly straightforward network structure, characterised by a \"bottleneck\" where the input is \"compressed\" before being upsampled again. This network can be used to create compressed representations of images by training the model to reconstruct the input on the output. It could also be used for our segmentation problem! However in segmentation, we don't really want our network to compress our image, we want it to do some \"work\" and then give us a segmented version of the input!\n",
    "<img src=\"https://miro.medium.com/max/3148/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "[Autoencoders](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOQv-PMmtxjW"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "data_path =\"Chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device to GPU_indx if GPU is avaliable\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create our Dataset</h2>\n",
    "Along with this notebook came a folder for our data which is split into folders, one containing the input images and the other the segmentation mask (label). Along with the data is a CSV file which specifies which data should be used for training and testing.The following dataset reads the test/train split CSV and saves the filenames of either the training or testing split.<br>\n",
    "There are many ways to construct such a dataset class as well as the format of your dataset folder structure, this is just one common way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6y7Gfkntxjc"
   },
   "outputs": [],
   "source": [
    "#Dataset class for our lung data (used by the data loader)\n",
    "class LungDataset(Dataset):\n",
    "    def __init__(self, root_data_dir, training = True):\n",
    "        \n",
    "        #Load all the filenames and their train/test indexs\n",
    "        #For every filename there is a 1 or a 0 indicating that it belongs to the \n",
    "        #training set or test set\n",
    "        train_test = np.loadtxt(root_data_dir + \"/Train_Test_split.csv\", dtype=\"<U8\")\n",
    "        \n",
    "        #Convert the indexs from str to int and compare this to \"training\"\n",
    "        #1 = True (Training)\n",
    "        #0 = False (Testing)\n",
    "        #Then use the boolean array to index the filenames\n",
    "        self.filenames = train_test[(train_test[:, 1].astype(int) == training), 0]\n",
    "        \n",
    "        #ToTensor object for converting image to tensor\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "        \n",
    "        self.root_data_dir = root_data_dir\n",
    "\n",
    "    #Returns a single image and label pair\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #Read image and labels\n",
    "        image = Image.open(self.root_data_dir + '/images/' + self.filenames[index])\n",
    "        label = Image.open(self.root_data_dir + '/labels/' + self.filenames[index])\n",
    "\n",
    "        #Make image in range (-1,1)\n",
    "        image = self.to_tensor(image)\n",
    "        image = (image-0.5)/0.5\n",
    "        image = image[0:1,:,:] #Images are grayscale, only need one channel\n",
    "        \n",
    "        #Cross entropy loss needs labels as LongTensor type\n",
    "        label = self.to_tensor(label).type(torch.LongTensor).squeeze(0)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH4eidyjtxjZ"
   },
   "source": [
    "<h3>Create a dataset and dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TlOmD35txje"
   },
   "outputs": [],
   "source": [
    "# Create train and test dataset\n",
    "dataset_train = LungDataset(data_path)\n",
    "dataset_test = LungDataset(data_path,False)\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zysqIam9txjh"
   },
   "source": [
    "<h3> Perform Sanity Check </h3>\n",
    "It is prudent to perform sanity check of the data correspondance. It become a routine check-up after a while but it is very crucial to check if we had made a mistake in loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3536,
     "status": "ok",
     "timestamp": 1568333935611,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "KCpRreK7txjh",
    "outputId": "3001c45a-dbf4-405f-e53b-fb05e4ce9144"
   },
   "outputs": [],
   "source": [
    "#create a dataloader itterable object\n",
    "dataloader_it = iter(data_loader_train)\n",
    "#sample from the itterable object\n",
    "image, label = next(dataloader_it)\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "img_out = torchvision.utils.make_grid(((image+1)/2)[0:4], 4)\n",
    "lbl_out = torchvision.utils.make_grid(label[0:4].unsqueeze(1), 4).float()\n",
    "\n",
    "out = torch.cat((img_out, lbl_out), 1)\n",
    "\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The U-Net</h2>\n",
    "The U-Net was developed specifically for image segmentation, the intuition being that the \"autoencoder-like\" structure will extract class information from the input image and the skip connections allow image \"structure\" information (contained in the feature maps) to jump the bottle-neck. This means that the network does not have to \"learn\" how to extract and compress the structure of the image leading to sharper edges and higher quality results.\n",
    "<img src=\"https://miro.medium.com/max/1200/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "[U-Net](https://towardsdatascience.com/u-net-b229b32b4a71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transpose Convolution</h3>\n",
    "The U-Net model also introduces a new layer-type the \"Transpose convolution\" (sometimes called \"Deconvolution\")<br>\n",
    "The transpose convolution is a \"learnable upsampling\" method and is essentially the opposite of a convolution! We take a single feature (pixel) in our feature map and replicate it and multiply by a kernel, any overlapping sections are added together. The easiest way to understand them is with the following animation (where the blue square is the input and green is the output).\n",
    "<img src=\"https://miro.medium.com/max/986/1*yoQ62ckovnGYV2vSIq9q4g.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "[Transpose Convolution](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8)\n",
    "\n",
    "[Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycO6FReFtxjk"
   },
   "source": [
    "<h4> Define the network - U-Net</h4>\n",
    "We will code the Unet model in two ways: Unet1 and Unet2.<br>\n",
    "Both of these two network structure are identical. The method we used for Unet1 is easier to visualise and understand, but Unet2 is modular, which allows easier adding/removing/modification of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIrWZN6maSAR"
   },
   "outputs": [],
   "source": [
    "# Unet1\n",
    "class Unet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(Unet1, self).__init__()\n",
    "        \n",
    "        # Define the first double conv layers, it contains\n",
    "        # 1. conv 32-channels out, 3x3 kernal and padding of 1\n",
    "        # The purpose of padding is to retain the output shape of each channel same as input.\n",
    "        # 2. add a nn.ReLU() to activate the conv layer\n",
    "        # 3. conv 32-channels out, 3x3 kernal and padding of 1\n",
    "        # 4. another relu\n",
    "        self.doubleconv1 = nn.Sequential (nn.Conv2d(1, 32, 3, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(32, 32, 3, padding=1),\n",
    "                                    nn.ReLU()\n",
    "                                   )\n",
    "        \n",
    "        # The second block contains a maxpooling and two conv layers followed by relu\n",
    "        # 1. maxpooling to halve the image size\n",
    "        # 2. conv 64-channels out, 3x3 kernal and padding of 1\n",
    "        # 3. relu\n",
    "        # 4. conv 64-channels out, 3x3 kernal and padding of 1\n",
    "        # 5. another relu\n",
    "        self.down2 = nn.Sequential (nn.MaxPool2d(2),\n",
    "                                    nn.Conv2d(32, 64, 3, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(64, 64, 3, padding=1),\n",
    "                                    nn.ReLU()\n",
    "                                   )\n",
    "        \n",
    "        # The third block:\n",
    "        # 1. maxpooling to halve the image size\n",
    "        # 2. conv 128-channels out, 3x3 kernal and padding of 1\n",
    "        # 3. relu\n",
    "        # 4. conv 128-channels out, 3x3 kernal and padding of 1\n",
    "        # 5. another relu\n",
    "        self.down3 =  nn.Sequential(nn.MaxPool2d(2),\n",
    "                                    nn.Conv2d(64, 128, 3, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(128, 128, 3, padding=1),\n",
    "                                    nn.ReLU()\n",
    "                                   )\n",
    "\n",
    "        \n",
    "        # Now as we reach the bottleneck of our network we want upsample to double the size of each feature map\n",
    "        # use nn.ConvTranspose2d with 64-channels out, 2x2 kernal and stride of 2\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        \n",
    "        # 1. conv 128-channels in, 64-channels out, 3x3 kernal and padding of 1\n",
    "        # the number of channels in is 128 is because we concatenate the output of self.up4\n",
    "        # and the output of self.down2\n",
    "        # 2. relu\n",
    "        # 3. conv 64-channels out, 3x3 kernal and padding of 1\n",
    "        # 4. another relu\n",
    "        self.doubleconv5 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(64, 64, 3, padding=1),\n",
    "                                         nn.ReLU()\n",
    "                                        )\n",
    "            \n",
    "        # use nn.ConvTranspose2d with 32-channels out, 2x2 kernal and stride of 2\n",
    "        self.up6 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        \n",
    "        # 1. conv 64-channels in, 32-channels out, 3x3 kernal and padding of 1\n",
    "        # 2. relu\n",
    "        # 3. conv 32-channels out, 3x3 kernal and padding of 1\n",
    "        # 4. another relu\n",
    "        # 5. conv 2-channels out, 1x1 kernal and no padding\n",
    "        self.doubleconv7 = nn.Sequential( nn.Conv2d(64, 32, 3, padding=1),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(32, 32, 3, padding=1),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(32, 2, 1, padding=0)\n",
    "                                        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the input to the network\n",
    "        # x is 1 x 64 x 64\n",
    "        x1 = self.doubleconv1(x)\n",
    "        # x1 = 32 x 64 x 64\n",
    "        x2 = self.down2(x1)\n",
    "        # x2 = 64 x 32 x 32\n",
    "        x3 = self.down3(x2)\n",
    "        # x3 = 128 x 16 x 16\n",
    "        x4 = self.up4(x3)\n",
    "        # x4 = 64 x 32 x 32\n",
    "        # torch.cat([x4,x2]) = 128 x 32 x 32 \n",
    "        x5 = self.doubleconv5(torch.cat([x4,x2],dim=1))\n",
    "        # x5 = 64 x 32 x 32\n",
    "        x6 = self.up6(x5)\n",
    "        # x6 = 32 x 64 x 64\n",
    "        # torch.cat([x6,x1]) = 64 x 64 x 64\n",
    "        x7 = self.doubleconv7(torch.cat([x6,x1],dim=1))\n",
    "        # x7 = 2 x 64 x 64\n",
    "    \n",
    "        return x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49zhhI_Ltxjl"
   },
   "outputs": [],
   "source": [
    "#Unet2\n",
    "class Unetdown(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, first_layer = False):\n",
    "        super(Unetdown, self).__init__()\n",
    "        \n",
    "        model = []\n",
    "        if not first_layer:\n",
    "            model += [nn.MaxPool2d(2)]\n",
    "        \n",
    "        model += [nn.Conv2d(input_nc, output_nc, 3, padding=1),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(output_nc, output_nc, 3, padding=1),\n",
    "                  nn.ReLU()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "\n",
    "class Unetup(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, last_layer = False):\n",
    "        super(Unetup, self).__init__()\n",
    "\n",
    "        self.up= nn.ConvTranspose2d(input_nc, output_nc, 2, stride=2)\n",
    "        model = []\n",
    "        model += [nn.Conv2d(input_nc, output_nc, 3, padding=1),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(output_nc, output_nc, 3, padding=1),\n",
    "                  nn.ReLU()]\n",
    "        \n",
    "        if last_layer:\n",
    "            model += [nn.Conv2d(output_nc, 2, 1, padding=0)]\n",
    "          \n",
    "        self.model = nn.Sequential(*model)\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        out = self.model(torch.cat([x1,x2],dim=1))\n",
    "        \n",
    "        return out\n",
    "            \n",
    "         \n",
    "class Unet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet2, self).__init__()\n",
    "        \n",
    "        self.down1 = Unetdown(1, 32, True)\n",
    "        self.down2 = Unetdown(32, 64, False)\n",
    "        self.down3 = Unetdown(64, 128, False)\n",
    "        self.up4 = Unetup(128, 64, False)\n",
    "        self.up5 = Unetup(64, 32, True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.up4(x3, x2)\n",
    "        x5 = self.up5(x4, x1)\n",
    "        \n",
    "        return x5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create our Model and optimizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7185,
     "status": "ok",
     "timestamp": 1568333950426,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "_Q6wuqf1txjn",
    "outputId": "2d270bda-da14-460b-9b0c-03fee37d8af1"
   },
   "outputs": [],
   "source": [
    "#Create UNet model - output is size (batch_size x 2 x H x W)\n",
    "#The two channels correspond to the two classes: not lung (class 0) and lung (class 1).\n",
    "\n",
    "#Transfor model to GPU\n",
    "model = Unet2().to(device)\n",
    "print(model)\n",
    "\n",
    "#Use an Adam optimiser to update the weights of the model\n",
    "optimiser = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "#Cross entropy - softmax over the two classes and negative log liklihood loss\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6I6gyaBtxjq"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43073,
     "status": "ok",
     "timestamp": 1568333987155,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "HR2Q73Ustxjr",
    "outputId": "199b5227-4868-4490-f308-56a213603b87"
   },
   "outputs": [],
   "source": [
    "#Set maximum epochs and create empty lists to store losses\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_test = 0.0\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    #For each training batch...\n",
    "    for i, (image, label) in enumerate(data_loader_train):   \n",
    "           \n",
    "        #Move images and labels to device\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        #Forward pass through model\n",
    "        outputs = model(image)\n",
    "        \n",
    "        #Compute cross entropy loss\n",
    "        loss = loss_fn(outputs, label)\n",
    "        running_loss_train += loss.item()\n",
    "\n",
    "        #Gradients are accumulated, so they should be zeroed before calling backwards\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        #Backward pass through model and update the model weights\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    running_loss_train /= len(data_loader_train)\n",
    "    Train_loss.append(running_loss_train)\n",
    "    \n",
    "    #Compute validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (image, label) in enumerate(data_loader_test):   \n",
    "               \n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = model(image)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            running_loss_test += loss.item()\n",
    "    \n",
    "        \n",
    "    running_loss_test /= len(data_loader_test)\n",
    "    Test_loss.append(running_loss_test)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    clear_output(True)\n",
    "    print('[Epoch {0:02d}] Train Loss: {1:.4f}, Val Loss: {2:.4f}, Time: {3:.4f}s'.format(epoch, running_loss_train, running_loss_test,end_time - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fheMmjnctxjt"
   },
   "source": [
    "## Plot the metric and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42194,
     "status": "ok",
     "timestamp": 1568333987155,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "VpFsKWPYtxju",
    "outputId": "4ab564c3-0620-4025-fd79-65435f32faf0"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(Train_loss, '-', label = 'Training Loss')\n",
    "plt.plot(Test_loss, '-', label = 'Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jq8bT-_Utxjw"
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42833,
     "status": "ok",
     "timestamp": 1568333988913,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "D0vPiL3ntxjx",
    "outputId": "16832fa3-ba62-4b03-92d2-6404d4238a04"
   },
   "outputs": [],
   "source": [
    "data_loader_iter = iter(data_loader_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        image, label = next(data_loader_iter)\n",
    "        \n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(image[0,0,:,:], cmap='gray')\n",
    "        plt.xlabel(\"Base Image\")\n",
    "        \n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output,dim=1,keepdim=True)\n",
    "        \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(label[0,:,:], cmap='gray')\n",
    "        plt.xlabel(\"Ground Truth\")\n",
    "        \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(pred.cpu().numpy()[0,0,:,:], cmap='gray')\n",
    "        plt.xlabel(\"Prediction\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chest_XRay_Segmentation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
