{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBNEsFZJPMS7"
   },
   "source": [
    "# Transfer Learning <br>\n",
    "Training a model from scratch can be time consuming and computationaly heavy. <br>\n",
    "In this notebook we look at how we can take a network trained on one dataset and use the learned weights as a step up, allowing us to achieve good results with little effort.<br>\n",
    "We will also look at techniques like data augmentation and learning rate decay to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sb01NHS5PMS8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUaeH517PMS_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # size of our mini batches\n",
    "num_epochs = 20  # How many itterations of our dataset\n",
    "learning_rate = 1e-4  # optimizer learning rate\n",
    "start_epoch = 0  # initialise what epoch we start from\n",
    "best_valid_acc = 0  # initialise best valid accuracy\n",
    "image_size = 96  # what to resize our images to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVGVcxx0PMTB"
   },
   "outputs": [],
   "source": [
    "save_checkpoint = False\n",
    "start_from_checkpoint = False\n",
    "save_dir = \"models\"\n",
    "model_name = \"Res_18_STL10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRJXAwTXPMTD"
   },
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "# GPU_indx = 0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.device(device)\n",
    "n_workers = 4 * torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wDY0BijPMTF"
   },
   "source": [
    "# Some preprocess to the dataset. eg: Convert the images to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keIwAFK-PMTG"
   },
   "outputs": [],
   "source": [
    "# Prepare a composition of transforms\n",
    "# all models from the Pytorch model Zoo where trained using images normalised with\n",
    "# the mean and std (one per channel) of the whole ImageNet Dataset\n",
    "# therefore the pretrained feature \"detectors\" of the model will expect the input to be normalized in the same way\n",
    "# https://pytorch.org/docs/stable/torchvision/models.html\n",
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjokNgt9PMTI"
   },
   "source": [
    "# Data Augmentation Transform<br>\n",
    "After training ResNet with the above transform record the results, then implement the transform with data augmentation below <br>\n",
    "With a small dataset our large model will more then likely simply overfit to (or memorize) the training data which will often lead to bad evaluation results<br>\n",
    "We can \"create more\" data from our limited dataset by applying random transformations as we sample images from our dataset instead of simply resizing them<br>\n",
    "By applying these transformations we are also forcing our model to generalise better to unseen images<br>\n",
    "You can also apply random affine transformations (shifts, scaling, rotations etc) - see Pytorch documentations <br>\n",
    "NOTE: you should only apply transforms that make sense, eg if at test time you'll never see an upside-down cat, don't flip your images vertically \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_R57HG8PMTJ"
   },
   "outputs": [],
   "source": [
    "# Prepare a composition of transforms\n",
    "# Replace the Resize in the above transform with two random transforms\n",
    "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "transform2 = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7L2lrkdPMTM"
   },
   "source": [
    "# Create the training, testing and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29503,
     "status": "ok",
     "timestamp": 1568947936500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "5FyAAqHWPMTM",
    "outputId": "d566a865-6439-47d3-a195-6b897199d923"
   },
   "outputs": [],
   "source": [
    "# Define our STL10 Datasets\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.STL10\n",
    "# Dataset definition is a bit differenet to MNIST and CIFAR10\n",
    "# STL10 has 3 different datasets, test, train and unlabeled\n",
    "# http://ai.stanford.edu/~acoates/stl10/\n",
    "# training set only has 5000 images and test set only 8000\n",
    "# Image size in this dataset are 96x96, larger then what we've been using\n",
    "# Try using transform 1 or 2 for the training set!! Only use transform1 for the test set!!\n",
    "data_dir = \"./data\"  # where to load/save the dataset from\n",
    "train_data = torchvision.datasets.STL10(\n",
    "    root=data_dir, split=\"train\", transform=transform2, download=True\n",
    ")\n",
    "test_data = torchvision.datasets.STL10(\n",
    "    root=data_dir, split=\"test\", transform=transform1, download=True\n",
    ")\n",
    "\n",
    "# Split trainging data into train and validation set with 90/10% traning/validation split\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = torch.utils.data.random_split(\n",
    "    train_data,\n",
    "    [n_train_examples, n_valid_examples],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPDubt3ZPMTO"
   },
   "source": [
    "# Check the lengths of all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29493,
     "status": "ok",
     "timestamp": 1568947936501,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "SrRt8srYPMTP",
    "outputId": "58f08b2c-0ff8-4ade-9102-aca69c0cef62"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1Y3cl77PMTR"
   },
   "source": [
    "# Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcSQsS_xPMTT"
   },
   "outputs": [],
   "source": [
    "# Create the training, Validation and Evaluation/Test Datasets\n",
    "# It is best practice to separate your data into these three Datasets\n",
    "# Though depending on your task you may only need Training + Evaluation/Test or maybe only a Training set\n",
    "# (It also depends on how much data you have)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTzyJvpwPMTX"
   },
   "outputs": [],
   "source": [
    "# This Function will allow us to scale an image's pixel values to a value between 0 and 1\n",
    "# It will undo the Normalisation that the Dataset performs\n",
    "def normalize_img(img):\n",
    "    mins = img.min(0, keepdims=True).min(1, keepdims=True)\n",
    "    maxs = img.max(0, keepdims=True).max(1, keepdims=True)\n",
    "    return (img - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbC_xRRFPMTZ"
   },
   "source": [
    "# Visualise the data <br>\n",
    "It is always important to fully understand what you are training your network with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30371,
     "status": "ok",
     "timestamp": 1568947937416,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "2ET6pMrYPMTa",
    "outputId": "9131cd8c-eab1-4a66-d4fc-1314ae775814"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "images, labels = next(iter(train_loader))\n",
    "out = torchvision.utils.make_grid(images[0:8])\n",
    "plt.imshow(normalize_img(out.numpy().transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGdAgnKgPMTc"
   },
   "source": [
    "# Create the pretrained network <br>\n",
    "First train the ResNet from scratch and collect the results for the training and evaluation accuracy and training time<br>\n",
    "Next set pretrained=True and train again collecting the results again <br>\n",
    "Next uncomment out the commented lines of code, this will stop the optimiser from updating the pretrained parts of the network \n",
    "<br><br>\n",
    "<b> Weight \"Freezing\"</b>\n",
    "<br>\n",
    "By \"freezing\" parts of the network like this we can speed up the training of the model as we will only be updating a single layer, this is especially useful if our pretrained model is very big (note we still have to do a full forward pass of the model which might take a while)<br>\n",
    "We can \"freeze\" the early layers of the model like this becasuse the ImageNet dataset that the model was trained on has similar images, and will have similar features to, the STL10 dataset we are using. Because of this the features in the images that the network would need to learn to detect, would be similar between datasets<br>\n",
    "NOTE if our dataset is very different to the ImageNet dataset \"freezing\" parts of the model might not be effective <br>\n",
    "Once we have trained our single layer for a while we can then unfreeze the rest of our model and train the whole thing for a few epochs to refine the model for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34657,
     "status": "ok",
     "timestamp": 1568947941813,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "JQPwhQuaPMTd",
    "outputId": "000d7dfc-5cd1-4afc-ef52-d7c1c7cc2754"
   },
   "outputs": [],
   "source": [
    "# Create a ResNet18 from the pytorch \"models\" module\n",
    "# This is reasonably sized model at 18 layers deep\n",
    "# ResNet Paper https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "# https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet18\n",
    "# res_net = models.resnet18(pretrained=True).to(device)\n",
    "res_net = models.resnet18(pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6CNrwv6PMTe"
   },
   "outputs": [],
   "source": [
    "# Uncomment this when ready\n",
    "# Loop through all the learnable parameter objects (from the layers)\n",
    "# for param in res_net.parameters():\n",
    "# #     Set to True to unfreeze layers\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNkCdO3iPMTg"
   },
   "source": [
    "Lets see the structure of this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1568947951609,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "41gzIJOXPMTh",
    "outputId": "03be971c-2b0c-4e18-bc41-6947b1850088"
   },
   "outputs": [],
   "source": [
    "# view the network\n",
    "res_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in res_net.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\n",
    "    f\"This model has {num_params} (approximately {num_params // 1e6} Million) Parameters!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEGEhnwhPMTj"
   },
   "source": [
    "The ImageNet challange dataset that the ResNet model was trained on has 1000 classes but the STL10 dataset only has 10 <br>\n",
    "We can still use the pretrained model we just need to alter it a bit by simply replacing the last FC (linear) layer with a new one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWcaOADhPMTk"
   },
   "outputs": [],
   "source": [
    "# Augment the model, by swapping out the last fc layer for a different one\n",
    "# get the number of in_features into the last fc layer\n",
    "num_ftrs = res_net.fc.in_features\n",
    "# redefine the last fc layer with a linear layer with 10 ouputs, this layer's weights will be randomly initialised\n",
    "res_net.fc = nn.Linear(num_ftrs, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1730,
     "status": "ok",
     "timestamp": 1568947953172,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "mf3F3XyrPMTl",
    "outputId": "022692d9-b195-4a91-bdcb-a0c31f474062"
   },
   "outputs": [],
   "source": [
    "# pass image through network\n",
    "out = res_net(images.to(device))\n",
    "# check output\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTzjoJDaPMTo"
   },
   "source": [
    "# Set up the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2K-lHdNPMTp"
   },
   "outputs": [],
   "source": [
    "# Pass our network parameters to the optimiser set our lr as the learning_rate\n",
    "optimizer = optim.Adam(res_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-CTnLcPMTt"
   },
   "outputs": [],
   "source": [
    "# Define a Cross Entropy Loss\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2hNmN7tPMTw"
   },
   "source": [
    "# Loading Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1568947953937,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "cOHZ6rjUPMTx",
    "outputId": "7ae52b87-5f01-420b-825c-cc8ce1335782"
   },
   "outputs": [],
   "source": [
    "# Create Save Path from save_dir and model_name, we will save and load our checkpoint here\n",
    "save_path = os.path.join(save_dir, model_name + \".pt\")\n",
    "\n",
    "# Create the save directory if it does note exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Load Checkpoint\n",
    "if start_from_checkpoint:\n",
    "    # Check if checkpoint exists\n",
    "    if os.path.isfile(save_path):\n",
    "        # load Checkpoint\n",
    "        check_point = torch.load(save_path)\n",
    "        # Checkpoint is saved as a python dictionary\n",
    "        # https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        # here we unpack the dictionary to get our previous training states\n",
    "        res_net.load_state_dict(check_point[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(check_point[\"optimizer_state_dict\"])\n",
    "        start_epoch = check_point[\"epoch\"]\n",
    "        best_valid_acc = check_point[\"valid_acc\"]\n",
    "        print(\"Checkpoint loaded, starting from epoch:\", start_epoch)\n",
    "    else:\n",
    "        # Raise Error if it does not exist\n",
    "        raise ValueError(\"Checkpoint Does not exist\")\n",
    "else:\n",
    "    # If checkpoint does exist and Start_From_Checkpoint = False\n",
    "    # Raise an error to prevent accidental overwriting\n",
    "    if os.path.isfile(save_path):\n",
    "        raise ValueError(\"Warning Checkpoint exists\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8cfC8aZPMT1"
   },
   "source": [
    "# Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHvhXRzwPMT2"
   },
   "outputs": [],
   "source": [
    "# This function should perform a single training epoch using our training data\n",
    "def train(net, device, loader, optimizer, loss_fun):\n",
    "\n",
    "    # initialise counters\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Set Network in train mode\n",
    "    net.train()\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "\n",
    "        # load images and labels to device\n",
    "        x = x.to(device)  # x is the image\n",
    "        y = y.to(device)  # y is the corresponding label\n",
    "\n",
    "        # Forward pass of image through network and get output\n",
    "        fx = net(x)\n",
    "\n",
    "        # Calculate loss using loss function\n",
    "        loss = loss_fun(fx, y)\n",
    "\n",
    "        # Zero Gradents\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagate Gradents\n",
    "        loss.backward()\n",
    "        # Do a single optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # create the cumulative sum of the loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        # log the loss for plotting\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "\n",
    "    # return the average loss from the epoch as well as the logger array\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wY9IAY2yPMT4"
   },
   "source": [
    "# Define the testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HnIWDYcPMT5"
   },
   "outputs": [],
   "source": [
    "# This function should perform a single evaluation epoch and will be passed our validation or evaluation/test data\n",
    "# it WILL NOT be used to train out model\n",
    "def evaluate(net, device, loader, loss_fun):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # Set network in evaluation mode\n",
    "    # Layers like Dropout will be disabled\n",
    "    # Layers like Batchnorm will stop calculating running mean and standard deviation\n",
    "    # and use current stored values\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "\n",
    "            # load images and labels to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward pass of image through network\n",
    "            fx = net(x)\n",
    "\n",
    "            # Calculate loss using loss function\n",
    "            loss = loss_fun(fx, y)\n",
    "\n",
    "            # calculate the accuracy\n",
    "            epoch_acc += (fx.argmax(1) == y).sum().item()\n",
    "\n",
    "            # log the cumulative sum of the loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "    epoch_acc /= len(loader.dataset)\n",
    "\n",
    "    # return the average loss and acc from the epoch as well as the logger array\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuDoBpbhPMT6"
   },
   "source": [
    "<h3> Learning rate schedular </h3>\n",
    "It can be useful to start with a high learning rate and then decrease it after some time allowing the optimiser to \"fine tune\" the model<br>\n",
    "There are many different ideas about how to change the learning rate over epochs, here we will create a simple \"linear decay\" schedular manually.<br>\n",
    "Pytorch also has automatic Learning rate scheduling\n",
    "\n",
    "[Learning rate scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAmicsKfPMT7"
   },
   "outputs": [],
   "source": [
    "# Create a function that will linearly decay the learning rate every epoch\n",
    "def lr_linear_decay(epoch_max, epoch, lr):\n",
    "    lr_adj = ((epoch_max - epoch) / epoch_max) * lr\n",
    "    # update the learning rate parameter of the optimizer\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRnG_8X9PMT8"
   },
   "source": [
    "# The training process <br>\n",
    "You should record the traning and evaluation accuracy as well as the training time after every experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekbPqq4nPMT9"
   },
   "outputs": [],
   "source": [
    "# Log the training and validation losses\n",
    "training_loss_logger = []\n",
    "validation_loss_logger = []\n",
    "# Log the training and validation losses\n",
    "training_acc_logger = []\n",
    "validation_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1568948678396,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "K27MEsO5PMT-",
    "outputId": "0c03f2f2-e250-4fad-dae5-b0dbaad8bda4"
   },
   "outputs": [],
   "source": [
    "# This cell implements our training loop\n",
    "\n",
    "# Record the start time\n",
    "Start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # Implement the linear decay of the learning rate\n",
    "    lr_linear_decay(num_epochs, epoch, learning_rate)\n",
    "\n",
    "    # call the training function and pass training dataloader etc\n",
    "    train_loss = train(res_net, device, train_loader, optimizer, loss_fun)\n",
    "\n",
    "    # call the evaluate function and pass validation/training dataloader etc\n",
    "    _, train_acc = evaluate(res_net, device, train_loader, loss_fun)\n",
    "    valid_loss, valid_acc = evaluate(res_net, device, valid_loader, loss_fun)\n",
    "\n",
    "    training_loss_logger.append(train_loss)\n",
    "    validation_loss_logger.append(valid_loss)\n",
    "\n",
    "    training_acc_logger.append(train_acc)\n",
    "    validation_acc_logger.append(valid_acc)\n",
    "    # If this model has the highest performace on the validation set\n",
    "    # then save a checkpoint\n",
    "    # {} define a dictionary, each entry of the dictionary is indexed with a string\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        if save_checkpoint:\n",
    "            print(\"Saving Model\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": res_net.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"valid_acc\": valid_acc,\n",
    "                },\n",
    "                save_path,\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:05.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:05.2f}% |\"\n",
    "    )\n",
    "\n",
    "End_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation accuracy was %.2f%%\" % (best_valid_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1568948455043,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "PeDwkZfzPMUC",
    "outputId": "9edb1db0-a6d9-4d8d-fb20-3b622c37120e"
   },
   "outputs": [],
   "source": [
    "print(\"Training time %.2f seconds\" % (End_time - Start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1568948455980,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "HoLp_P3xPMUE",
    "outputId": "b241900f-ff45-42f0-dc33-14b48126836f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "train_x = np.linspace(0, num_epochs, len(training_loss_logger))\n",
    "plt.plot(train_x, training_loss_logger, c=\"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(validation_loss_logger))\n",
    "plt.plot(valid_x, validation_loss_logger, c=\"k\")\n",
    "\n",
    "plt.title(\"ResNet Loss\")\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "train_x = np.linspace(0, num_epochs, len(training_acc_logger))\n",
    "plt.plot(train_x, training_acc_logger, c=\"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(validation_acc_logger))\n",
    "plt.plot(valid_x, validation_acc_logger, c=\"k\")\n",
    "\n",
    "plt.title(\"ResNet Acc\")\n",
    "plt.legend([\"Training Acc\", \"Validation Acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_F2Qy9WPMUG"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1568948469315,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "dKMx57tEPMUH",
    "outputId": "7590031a-2a9e-4701-9799-320155e5efd6"
   },
   "outputs": [],
   "source": [
    "# call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_loss, test_acc = evaluate(res_net, device, test_loader, loss_fun)\n",
    "print(\"Testing: | Loss %.2f | Accuracy %.2f%% |\" % (test_loss, 100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet18_STL10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
