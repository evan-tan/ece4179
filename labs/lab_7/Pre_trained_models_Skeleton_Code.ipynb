{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSKNzBkCPLyg",
    "tags": []
   },
   "source": [
    "<h1> Trained model Manipulation </h1>\n",
    "In this lab we are going to see what we can do with a pre-trained classifier model (other than classify images) and hopefully get a better idea of what is going on inside our models!<br>\n",
    "First we will try and visulise what our traied network is \"looking\" at when it makes a classification <br>\n",
    "To do this we are going to take a pre-trained model from pytorch's \"Model Zoo\", VGG19 in this case, and backprop the gradients from a single output to the input image and visulise the magnitudes of the gradients <br>\n",
    "Next we will look what happens when we change our input image with these gradients\n",
    "<img src=\"https://glassboxmedicine.files.wordpress.com/2019/06/greater-swiss-mountain-dog.jpeg\" width=\"800\" align=\"center\">\n",
    "\n",
    "[CAM](https://glassboxmedicine.com/2019/06/11/cnn-heat-maps-class-activation-mapping-cam/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0X8hmv9PLyi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as models\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVo2XEI2PLyl"
   },
   "outputs": [],
   "source": [
    "# Set device to GPU if avaliable\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "000YHUETPLyn"
   },
   "source": [
    "The VGG19 model we will be using was trained using the ImageNet challenge dataset, let's load a file of the class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fpe9IJFePLyo"
   },
   "outputs": [],
   "source": [
    "# Load a list of the 1000 ImageNet classes from the ImageNet challenge\n",
    "# https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "image_net_classes = np.loadtxt(\"Imagenet_classes.csv\", dtype=str, delimiter=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_classes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivvpyi1CPLyq"
   },
   "source": [
    "Load our test image to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzcIp6ydPLyq"
   },
   "outputs": [],
   "source": [
    "# Load our test image\n",
    "test_img = Image.open(\"Pupper.jpg\").convert(\"RGB\")\n",
    "# Transform the PIL image to a tensor and normalize using the means and std used to train the VGG16 model\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(512),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "# Make sure you add on the batch dimension\n",
    "test_img1 = transform(test_img).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzDq3TBDPLys"
   },
   "source": [
    "A few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRBXc8XOPLyt"
   },
   "outputs": [],
   "source": [
    "# This Function will allow us to scale an images pixel values to a value between 0 and 1\n",
    "def normalize_img(img):\n",
    "    mins = img.min(0, keepdims=True).min(1, keepdims=True)\n",
    "    maxs = img.max(0, keepdims=True).max(1, keepdims=True)\n",
    "    return (img - mins) / (maxs - mins)\n",
    "\n",
    "\n",
    "# This clip function forces the input to be within the range to be within the max and min of an image\n",
    "# normalised with the given mean and std (from an initial range of 0-1)\n",
    "def clip(image_tensor):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    for c in range(3):\n",
    "        m, s = mean[c], std[c]\n",
    "        # clip the input to be within the min and max values\n",
    "        image_tensor[0, c] = torch.clamp(image_tensor[0, c], -m / s, (1 - m) / s)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufvLyIfnPLyv"
   },
   "source": [
    "# Create a pretrained VGG19 Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13163,
     "status": "ok",
     "timestamp": 1568942598353,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "uWmOvXKKPLyv",
    "outputId": "f930e218-2db1-4748-891d-766cbf1e2037"
   },
   "outputs": [],
   "source": [
    "# Create a VGG19 from the pytorch \"models\" module and download the pre trained weights\n",
    "# https://pytorch.org/docs/stable/torchvision/models.html\n",
    "# These models have be trained on the ImageNet challenge dataset (1.3 million images, 1000 classes) to a reasonably high accuracy\n",
    "vgg_net = models.vgg19(pretrained=True).to(device)\n",
    "# We're not training it so put it in eval mode\n",
    "vgg_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in vgg_net.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(f\"This model has approximately {num_params} Parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LOTnB-FPLyy"
   },
   "source": [
    "Visulise the shape of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12865,
     "status": "ok",
     "timestamp": 1568942598354,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "rwqbyUpZPLyz",
    "outputId": "9e673530-9b65-4676-931b-79055ad95d3a"
   },
   "outputs": [],
   "source": [
    "vgg_net(test_img1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1l29pKwLPLy1"
   },
   "source": [
    "<h3> Visulise test image!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14068,
     "status": "ok",
     "timestamp": 1568942600575,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "A4Df2MNTPLy1",
    "outputId": "19da5573-4166-4220-80dc-a4ff5cbb7339"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "np_img = test_img1[0].cpu().numpy().transpose((1, 2, 0))\n",
    "image_norm = normalize_img(np_img)\n",
    "plt.imshow(image_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNFASTlUPLy4"
   },
   "source": [
    "<h3>  What does VGG19 think our test image is?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12687,
     "status": "ok",
     "timestamp": 1568942600577,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "vfDjVpJtPLy4",
    "outputId": "87e5635c-e9a9-4518-d4ef-e91356931af2"
   },
   "outputs": [],
   "source": [
    "# Get the index of the max ouput of the network\n",
    "idx = vgg_net(test_img1).argmax(1).item()\n",
    "# Use this to index the class list to get the clas name\n",
    "print(f\"This image is class {idx} which is a {image_net_classes[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I04ax_eiPLy6"
   },
   "source": [
    "<h3> What are you looking at???</h3>\n",
    "Now that we know what it thinks it is, we can try to work out what part of the image VGG19 has used to make it's decision by simply looking at the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeohN5QxPLy7"
   },
   "outputs": [],
   "source": [
    "# make a copy of our test image and use it to create an autograd variable so that we can capture the gradients\n",
    "image = copy.deepcopy(test_img1)\n",
    "image.requires_grad = True\n",
    "# Get the index of the max ouput of the network\n",
    "output = vgg_net(image)\n",
    "# Backpropagate the gradients from the max output to the input image\n",
    "# In this way we are calculating the how the different input pixels of our image affect the output\n",
    "# You can actually backprop from anywhere in your network!\n",
    "# NOTE we can only backpropagate from a single value\n",
    "output[0, idx].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dlIs-d8PLy9"
   },
   "outputs": [],
   "source": [
    "# TODO: Fix this cell\n",
    "# Copy the gradients and flatten into a 2D tensor by taking the max along the channels\n",
    "grad_values, _ = image.grad.detach()[0].cpu().abs().max(0)\n",
    "# Downsample then upsample as a quick and dirty way of generating a heatmap\n",
    "grad_scale = F.avg_pool2d(grad_values.unsqueeze(0), 10).unsqueeze(0)\n",
    "grad_scale = (\n",
    "    F.upsample_bilinear(grad_scale, size=(grad_values.shape[0], grad_values.shape[1]))\n",
    "    .squeeze(0)\n",
    "    .squeeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdTAEmJvPLy_"
   },
   "source": [
    "<h3> Visulise </h3>\n",
    "This method is a crude way of visulising what the network is paying attention to, brighter areas correspond to higher gradients <br>\n",
    "If you are interested, checkout this implementation of Class Activation Mapping (CAM) for a better method <br> \n",
    "\n",
    "[Class Activation Mapping](http://snappishproductions.com/blog/2018/01/03/class-activation-mapping-in-pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2293,
     "status": "ok",
     "timestamp": 1568942666718,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "pSmWFX_bPLy_",
    "outputId": "d5378abf-ff96-4a99-9de1-e9474ed8ea2f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grad_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcmKSS2xPLzC"
   },
   "source": [
    "<h2> Generating Art </h2>\n",
    "Once we get the gradients of our image with respect to the ouput, what can we do with them? <br>\n",
    "These gradients tell us how to change the input image to INCREASE a given output (or wherever you backproped from) <br>\n",
    "So what happens when we use these gradients to change our image?\n",
    "\n",
    "\n",
    "<img src=\"https://b2h3x3f6.stackpathcdn.com/assets/landing/img/gallery/4.jpg\" width=\"800\" align=\"center\">\n",
    "\n",
    "[Deep Dream Generator](https://deepdreamgenerator.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1568943412757,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "QV-83DIwPLzE",
    "outputId": "b54807f3-edda-4c0a-a646-f4835a2c1d94"
   },
   "outputs": [],
   "source": [
    "class_indx = 33\n",
    "print(\"This class is a\", image_net_classes[class_indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsEbGdjEPLzD"
   },
   "source": [
    "Lets Select an ImageNet class, we don't have to backpropagate from the real class, infact we can backprop from any feature anywhere in our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI_g7LJPPLzF"
   },
   "source": [
    "Like before lets make another copy of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdi91pUePLzG"
   },
   "outputs": [],
   "source": [
    "image2 = copy.deepcopy(test_img1)\n",
    "image2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUmvWj8LPLzI"
   },
   "outputs": [],
   "source": [
    "# Define a Learning or \"update\" rate\n",
    "# try dropping learning rate in order to let the network\n",
    "# think that it is a turtle, without any noticeable change\n",
    "# lr = 0.05 # still see some scales\n",
    "lr = 1.5  # imperceptible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIvqIdCTPLzK"
   },
   "source": [
    "We will now backprop from the class activation indexed by the class we chose eariler <br>\n",
    "Using the gradients collected we will take a \"step\" in the direction of the gradient by adding the gradient to our image <br>\n",
    "As a result we will be enhancing any features of our image that look like they belong to our chosen class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBNzqwNjPLzK"
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    # Forward pass of network\n",
    "    out = vgg_net(image2)\n",
    "    # you don't really need to zero the gradients of the network as we don't use them\n",
    "    vgg_net.zero_grad()\n",
    "    # Backprop from chosen class activation\n",
    "    out[0, class_indx].backward()\n",
    "    # update the image with the scaled gradients\n",
    "    # do gradient ascent\n",
    "    image2.data += lr * image2.grad.data\n",
    "    # clip the image to keep the pixel values within the origional range\n",
    "    image2.data = clip(image2.data)\n",
    "    # we should techinically zero the gradients of the image so they don't accumulate over multiple iteration\n",
    "    # but in practice for this application it does not make much a difference\n",
    "    image2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JwSk10JPLzP"
   },
   "source": [
    "Now that we've updated our image, what does VGG19 think our image is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1568943433171,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "HRc9A0QYPLzP",
    "outputId": "16f3c036-55ff-44eb-8b7b-aadc2b1bce85"
   },
   "outputs": [],
   "source": [
    "indx = vgg_net(image2).argmax(1).item()\n",
    "print(\"This image is now a\", image_net_classes[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jXhhVNPPLzS"
   },
   "source": [
    "Let's visualize our altered image!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3396,
     "status": "ok",
     "timestamp": 1568943458572,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "5wWEKViSPLzT",
    "outputId": "76617cec-71f5-4acd-b139-c05aef777a5c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "np_img = image2[0].detach().cpu().numpy().transpose((1, 2, 0))\n",
    "image_norm = normalize_img(np_img)\n",
    "plt.imshow(image_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using Multiple Scales</h3>\n",
    "As we can see in our altered image the updates have mainly changed fine details of the image, this is because many of the layers of the network only opperate on small regions of the image, it is only the final layers' \"receptive field\" that encompasses the whole image. If we want to make large scale changes to our image (aka modify the general shape of objects in the image) we need more layers of our network to \"view\" larger regions of the image. We can do this by simply downsampling our input image, however this means the resolution of the ouput image will be low. Instead we can perform some steps of the gradient ascent on the low-res image and then upsample the modified image and again perform gradient ascent. By doing this we can make start by making large scale changes to the image and then make finer and finer changes. We can perform this usampling multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Learning or \"update\" rate\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = copy.deepcopy(test_img1)\n",
    "# downsample the image by a factor of 8\n",
    "image3 = F.avg_pool2d(image3, 8)\n",
    "image3.requires_grad = True\n",
    "\n",
    "for _ in range(6):\n",
    "    for _ in range(10):\n",
    "        # Forward pass of network\n",
    "        out = vgg_net(image3)\n",
    "        vgg_net.zero_grad()\n",
    "        # Backprop from chosen class activation\n",
    "        out[0, class_indx].backward()\n",
    "        # update the image with the scaled gradients\n",
    "        # do gradient ascent\n",
    "        image3.data += lr * image3.grad.data\n",
    "        # clip the image to keep the pixel values within the origional range\n",
    "        image3.data = clip(image3.data)\n",
    "        image3.grad.data.zero_()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image3 = F.upsample_bilinear(image3, scale_factor=1.25)\n",
    "    image3.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "np_img = image3[0].detach().cpu().numpy().transpose((1, 2, 0))\n",
    "image_norm = normalize_img(np_img)\n",
    "plt.imshow(image_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfZalSoFPLzV"
   },
   "source": [
    "<h3> Slicing our network</h3>\n",
    "This VGG19 implementation is mainly made up of two nn.sequential blocks <br>Lets only take one of them, the initial \"features\" block, and from it only take some of the initial layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1568943659205,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "lY8vpDOA5q8v",
    "outputId": "7611a482-0a46-4125-a239-e3b5afd10dc4"
   },
   "outputs": [],
   "source": [
    "vgg_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNG_bzh6PLzV"
   },
   "outputs": [],
   "source": [
    "# copy the features block, and take all layers from the first to the 10th last layer\n",
    "# take the feature extractor\n",
    "features_net = vgg_net.features[:]\n",
    "features_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71zHGymjPLzX"
   },
   "outputs": [],
   "source": [
    "image4 = copy.deepcopy(test_img1)\n",
    "image4 = F.avg_pool2d(image4, 8)\n",
    "image4.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1568943660802,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "p8FZJGJYPLzZ",
    "outputId": "88d90fcb-aac0-40b9-cae9-c810597a9812"
   },
   "outputs": [],
   "source": [
    "# what do the feature maps at this layer look like?\n",
    "features_net(image4).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knak7ACGPLza"
   },
   "source": [
    "Lets update our image by backproping from the mean of a single feature map (channel) of the last layer of our \"features\" block <br>\n",
    "What would happen if we only backproped from only one feature in this layer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77T7spKIPLzb"
   },
   "outputs": [],
   "source": [
    "channel_id = 3\n",
    "for _ in range(6):\n",
    "    for _ in range(50):\n",
    "        # Forward pass of network\n",
    "        out = features_net(image4)\n",
    "        features_net.zero_grad()\n",
    "        # Backprop from chosen class activation\n",
    "        out[0, channel_id].mean().backward()\n",
    "        # update the image with the scaled gradients\n",
    "        image4.data += lr * image4.grad.data\n",
    "        # clip the image to keep the pixel values within the origional range\n",
    "        image4.data = clip(image4.data)\n",
    "        image4.grad.data.zero_()\n",
    "    with torch.no_grad():\n",
    "        image4 = F.upsample_bilinear(image4, scale_factor=1.3)\n",
    "    image4.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1859,
     "status": "ok",
     "timestamp": 1568944475224,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "vHWm3DMWPLzc",
    "outputId": "d5ee8924-8a14-401e-aedf-6e457e91a9bc"
   },
   "outputs": [],
   "source": [
    "indx = vgg_net(image4).argmax(1).item()\n",
    "print(\"This image is now a\", image_net_classes[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By backpropagating from an earlier layer in our network we exagerating \"lower-level\" features of our image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "np_img = image4[0].detach().cpu().numpy().transpose((1, 2, 0))\n",
    "image_norm = normalize_img(np_img)\n",
    "plt.imshow(image_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using target features</h3>\n",
    "Instead of maximising random features of our image, let instead get the features of a \"target\" image at some layer of our network and make the features of our source image match them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our target image\n",
    "test_img2 = Image.open(\"pattern.jpg\").convert(\"RGB\")\n",
    "test_img2 = transform(test_img2).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "np_img = test_img2[0].detach().cpu().numpy().transpose((1, 2, 0))\n",
    "image_norm = normalize_img(np_img)\n",
    "plt.imshow(image_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = vgg_net(test_img2).argmax(1).item()\n",
    "print(\"This image is now a\", image_net_classes[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another copy of the features block, and take all layers from the first to the 20th last layer\n",
    "features_net_sliced = vgg_net.features[:-1]\n",
    "features_net_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean value for each of the channels for our target image\n",
    "target_features = features_net_sliced(test_img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of feature maps for every channel\n",
    "target_features.mean(dim=[2, 3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image5 = copy.deepcopy(test_img1)\n",
    "image5 = F.avg_pool2d(image5, 8)\n",
    "image5.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(6):\n",
    "    for _ in range(50):\n",
    "        # Forward pass of network\n",
    "        out = features_net_sliced(image5)\n",
    "        features_net_sliced.zero_grad()\n",
    "        # Update our source image so the mean features at this layer match the target image\n",
    "        current = out.mean(dim=[2, 3])\n",
    "        target = target_features.mean(dim=[2, 3])\n",
    "        (current - target).pow(2).mean().backward(retain_graph=True)\n",
    "\n",
    "        # update the image with the scaled gradients\n",
    "        image5.data -= lr * image5.grad.data\n",
    "        # clip the image to keep the pixel values within the origional range\n",
    "        image5.data = clip(image5.data)\n",
    "        image5.grad.data.zero_()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image5 = F.upsample_bilinear(image5, scale_factor=1.3)\n",
    "\n",
    "    image5.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "np_img = image5[0].detach().cpu().numpy().transpose((1, 2, 0))\n",
    "image_norm = normalize_img(np_img)\n",
    "plt.imshow(image_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepDream.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
