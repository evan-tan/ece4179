{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNorm\n",
    "\n",
    "In this demo, we make use of the BatchNorm (BN) to improve a two layer MLP. We first start by loading necessary packages and data. We use the Fashion MNIST dataset in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# get the training and test datasets\n",
    "trainset = datasets.FashionMNIST(root='../data', train=True, download=False, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='../data', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize a few samples to get a feeling about the dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
    "images, _ = next(iter(train_loader))\n",
    "\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "#display 10 images in batch\n",
    "\n",
    "grid = torchvision.utils.make_grid(images, nrow = 10)\n",
    "plt.figure(figsize= (15,15))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define our MLP. For fully connected (fc) layers, we use the PyTorch layer BatchNorm1d. If you want to apply the BN to convolutional layers, you need to use BatchNorm2d. Study the code below. Note that we put the BN before the activation function, following general practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, use_batch_norm=True,  hidden_dim=256):\n",
    "\n",
    "        super(MLP, self).__init__() # init super\n",
    "        \n",
    "        # Default layer sizes\n",
    "        self.input_size = 784 # (28*28 images)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # define hidden linear layers, with optional batch norm on their outputs\n",
    "        # layers with batch_norm applied have no bias term\n",
    "        if use_batch_norm:\n",
    "            self.fc1 = nn.Linear(self.input_size, hidden_dim*2, bias=False)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(self.input_size, hidden_dim*2)\n",
    "            \n",
    "        # define *second* hidden linear layers, with optional batch norm on their outputs\n",
    "        if use_batch_norm:\n",
    "            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim, bias=False)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        \n",
    "        # third and final, fully-connected layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # flatten image\n",
    "        x = x.view(-1, self.input_size)\n",
    "        # all hidden layers + optional batch norm + relu activation\n",
    "        x = self.fc1(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        # third layer, no batch norm or activation\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions below are used for training and evaluating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, device=\"cpu\"):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_idx, (imgs, labels) in enumerate(train_loader): \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()        \n",
    "        running_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return (running_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device=\"cpu\"):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            outputs = model(data.to(device))                     \n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            correct_predictions += (predicted == target.to(device)).sum().item()\n",
    "            total_predictions += target.shape[0]\n",
    "        return (100*correct_predictions/total_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate two models below, one with and one without BN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "MLP_plain = MLP(use_batch_norm=False, hidden_dim=256).to(device)\n",
    "MLP_BN = MLP(use_batch_norm=True, hidden_dim=256).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train the model without BN for 10 epochs below and store the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "\n",
    "# OPTIMISER PARAMETERS\n",
    "lr = 0.01 \n",
    "optimizer_plain = torch.optim.SGD(MLP_plain.parameters(), lr=lr)\n",
    "optimizer_BN = torch.optim.SGD(MLP_BN.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "result_file_plain = 'results/mlp_plain.csv'\n",
    "model_file_plain = 'models/mlp_plain.pt'\n",
    "cols       = ['epoch', 'train_loss', 'train accuracy', 'test accuracy', 'total training time']\n",
    "results_df_plain = pd.DataFrame(columns=cols).set_index('epoch')\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "training_time_plain = 0\n",
    "best_test_acc_plain = 0.0\n",
    "for epoch in range(max_epoch): \n",
    "\n",
    "    #plain model\n",
    "    start_time = timer()\n",
    "    train_loss = train_model(MLP_plain, optimizer_plain, criterion, trainloader, device)\n",
    "    end_time = timer()\n",
    "    training_time_plain += (end_time-start_time)\n",
    "    \n",
    "    #evaluating\n",
    "    train_acc = evaluate_model(MLP_plain, trainloader, device)\n",
    "    test_acc = evaluate_model(MLP_plain, testloader, device)\n",
    "    print(f'Plain MLP - Epoch:{epoch+1:3}| training loss = {train_loss:.4f}|', \n",
    "          f'Training Accuracy = {train_acc:.2f}| Test Accuracy = {test_acc:.2f}|', \n",
    "          f'Training Time (sec) = {training_time_plain:.2f}|')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    results_df_plain.loc[epoch] = [train_loss, train_acc, test_acc, training_time_plain]\n",
    "    results_df_plain.to_csv(result_file_plain, float_format='%.2f')\n",
    "    # Save best model\n",
    "    if (test_acc > best_test_acc_plain):\n",
    "            torch.save(MLP_plain.state_dict(),model_file_plain)\n",
    "            best_test_acc_plain = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model with BN for 10 epochs and store the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================= \n",
    "result_file_bn = 'results/mlp_bn.csv'\n",
    "model_file_bn = 'models/mlp_bn.pt'     \n",
    "results_df_bn = pd.DataFrame(columns=cols).set_index('epoch')         \n",
    "# prepare data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False) \n",
    " \n",
    "training_time_bn = 0  \n",
    "best_test_acc_bn = 0.0\n",
    "for epoch in range(max_epoch):     \n",
    "    #batchnorm model\n",
    "    start_time = timer()\n",
    "    train_loss = train_model(MLP_BN, optimizer_BN, criterion, trainloader, device)\n",
    "    end_time = timer()\n",
    "    training_time_bn += (end_time-start_time)\n",
    "    \n",
    "    #evaluating\n",
    "    train_acc = evaluate_model(MLP_BN, trainloader, device)\n",
    "    test_acc = evaluate_model(MLP_BN, testloader, device)\n",
    "\n",
    "    print(f'MLP with BN - Epoch:{epoch+1:3}| training loss = {train_loss:.4f}|' \n",
    "          f'Training Accuracy = {train_acc:.2f}| Test Accuracy = {test_acc:.2f}|' \n",
    "          f'Training Time (sec) = {training_time_bn:.2f}|')\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    results_df_bn.loc[epoch] = [train_loss, train_acc, test_acc, training_time_bn]\n",
    "    results_df_bn.to_csv(result_file_bn, float_format='%.2f')\n",
    "    # Save best model\n",
    "    if (test_acc > best_test_acc_bn):\n",
    "            torch.save(MLP_BN.state_dict(),model_file_bn)\n",
    "            best_test_acc_bn = test_acc   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to compare and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = [ 'c',  'r']\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(25, 7))\n",
    "\n",
    "plain_net_df = pd.read_csv(f'results/mlp_plain.csv')\n",
    "BN_net_df = pd.read_csv(f'results/mlp_bn.csv')\n",
    "\n",
    "\n",
    "ax1.axis([0, 10, 70, 100])\n",
    "    \n",
    "\n",
    "ax1.plot(plain_net_df['epoch'], plainnet_df['train accuracy'], color='yellowgreen',\n",
    "                 linestyle='--', label=f'MLP - train')\n",
    "ax1.plot(BN_net_df['epoch'], resnet_df['train accuracy'], color='darkmagenta',\n",
    "                 linestyle='--', label=f'MLP w/ BN - train')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "ax1.set_title('Train Accuracy')\n",
    "ax1.legend(loc='lower left')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('accuracy (%)')\n",
    "ax1.axhline(10, color='black', alpha=0.5, dashes=(10., 10.))\n",
    "ax1.axhline(5, color='black', alpha=0.5, dashes=(10., 10.))\n",
    "    \n",
    "\n",
    "\n",
    "ax1.axis([0, 10, 70, 100])\n",
    "    \n",
    "ax2.plot(plain_net_df['epoch'], plainnet_df['test accuracy'], color='yellowgreen',\n",
    "                 linestyle='--', label=f'MLP - test')\n",
    "ax2.plot(BN_net_df['epoch'], resnet_df['test accuracy'], color='darkmagenta',\n",
    "                 linestyle='--', label=f'MLP w/ BN test')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "ax2.set_title('Test Accuracy')\n",
    "ax2.legend(loc='lower left')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('accuracy (%)')\n",
    "ax2.axhline(10, color='black', alpha=0.5, dashes=(10., 10.))\n",
    "ax2.axhline(5, color='black', alpha=0.5, dashes=(10., 10.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- Implement a CNN with and without BN and compare the results. \n",
    "- Check the ResNet block. Do we use BN with ResNet? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
