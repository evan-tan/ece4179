{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 4 * torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 dataloader\n",
    "\n",
    "the following class reads the data for Q3 and creates a torch dataset object for it. With this, you can easily \n",
    "use a dataloader to train your model. \n",
    "\n",
    "Make sure that the file \"hw2_Q3_data.npz\" is located properly (in this example, it should be in the same folder as this notebook.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q3Data(Dataset):\n",
    "    def __init__(self, trn_tst=0):\n",
    "        data = np.load(\"./data/hw2_Q4_data.npz\")\n",
    "        if trn_tst == 0:\n",
    "            # trainloader\n",
    "            self.images = data[\"arr_0\"].T\n",
    "            self.labels = data[\"arr_1\"]\n",
    "        else:\n",
    "            # testloader\n",
    "            self.images = data[\"arr_2\"].T\n",
    "            self.labels = data[\"arr_3\"]\n",
    "\n",
    "        self.images = np.float32(self.images) / 255.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.images[idx, :]\n",
    "        labels = self.labels[idx]\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how you can create a dataloader for the Q3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Q3Data(trn_tst=0)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "testset = Q3Data(trn_tst=1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log(log, model_config, select=False, save=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    if select:\n",
    "        # find min/max for criteria\n",
    "        selected = {}\n",
    "        for key in log:\n",
    "            if \"loss\" in key:\n",
    "                idx = np.argmin(log[key][9::10])\n",
    "                label = \"Min \"\n",
    "            elif \"acc\" in key:\n",
    "                idx = np.argmax(log[key][9::10])\n",
    "                label = \"Max \"\n",
    "            # 10 - 2000 epochs would be saved as 9 - 1999\n",
    "            # take every 10th epoch, determine a usable model\n",
    "            actual_idx = (idx + 1) * model_config[\"save_interval\"] - 1\n",
    "            selected[key] = actual_idx\n",
    "\n",
    "            print(key, actual_idx)\n",
    "            print(actual_idx, log.get(key)[actual_idx])\n",
    "\n",
    "            ax.plot(\n",
    "                actual_idx,\n",
    "                log.get(key)[actual_idx],\n",
    "                label=label + key,\n",
    "                markersize=16,\n",
    "                marker=\"X\",\n",
    "            )\n",
    "\n",
    "    epochs = model_config.get(\"num_epochs\")\n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "\n",
    "    ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "    ax.plot(x_axis, log.get(\"acc\"), label=\"Test Accuracy\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(16)\n",
    "\n",
    "    ax.legend(loc=\"best\", prop={\"size\": 12})\n",
    "    if save:\n",
    "        plt.savefig(f\"./LR_{model_config['lr']}_{model_config['num_epochs']}.jpg\")\n",
    "    plt.show()\n",
    "    if select:\n",
    "        return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, data_generator, loss_fn):\n",
    "    \"\"\"Function to easily test model on specified dataset\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_loss, batch_steps = 0.0, 0\n",
    "        correct_pred, total_pred = 0, 0\n",
    "\n",
    "        for batch_id, (data, label) in enumerate(data_generator):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            batch_loss += loss_fn(output, label).item()\n",
    "            batch_steps += 1\n",
    "\n",
    "            # indices where probability is maximum\n",
    "            _, val_pred = torch.max(output, 1)\n",
    "\n",
    "            correct_pred += (val_pred == label).sum().item()\n",
    "            total_pred += label.shape[0]\n",
    "\n",
    "        # average loss/acc across ALL batches\n",
    "        # i.e. ACROSS specified dataset\n",
    "        avg_loss = batch_loss / batch_steps\n",
    "        avg_acc = correct_pred / total_pred\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"The original training function has been modified in order to use Ray's Tune\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_workers = 4 * torch.cuda.device_count()\n",
    "\n",
    "    logger = {\n",
    "        \"train_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"acc\": np.zeros(config[\"num_epochs\"]),\n",
    "    }\n",
    "\n",
    "    #### LOAD DATA ####\n",
    "    b_size = config[\"batch_size\"]\n",
    "\n",
    "    train_data = Q3Data(0)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    test_data = Q3Data(1)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    #### INSTANTIATE MODEL ####\n",
    "    net = config[\"model\"].to(device)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    if config[\"lr_variable\"]:\n",
    "        # what approximate epoch does convergence occur? 80 in this case\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1)\n",
    "\n",
    "    #### BEGIN TRAINING ####\n",
    "    start_time = time.time()\n",
    "    for j in range(config[\"num_epochs\"]):\n",
    "        ## START OF BATCH ##\n",
    "        train_loss, train_steps = 0.0, 0\n",
    "        for batch_id, (data, label) in enumerate(train_dataloader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "\n",
    "            loss = loss_function(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        ## END OF BATCH ##\n",
    "\n",
    "        # average training loss for 1 epoch\n",
    "        train_loss /= train_steps\n",
    "        _, test_acc = test_model(net, test_dataloader, loss_function)\n",
    "\n",
    "        logger[\"train_loss\"][j] = train_loss\n",
    "        logger[\"acc\"][j] = test_acc\n",
    "\n",
    "        if config[\"log_training\"] and (j + 1) % config[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                f\"Epoch:{j+1}/{config['num_epochs']} \\\n",
    "                Train Loss: {logger['train_loss'][j]:.6f} Test Acc: {logger['acc'][j]:.6f}\"\n",
    "            )\n",
    "\n",
    "        # make sure folder is created to place saved checkpoints\n",
    "        path = Path.cwd() / \"models\" / net._name\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        if config[\"save_model\"] and (j + 1) % config[\"save_interval\"] == 0:\n",
    "            lr_str = \"VarLR\" if config[\"lr_variable\"] else \"FixedLR\"\n",
    "            # pad with appropriate number of zeros i.e. epoch 10 named as 010\n",
    "            checkpoint_num = str(j + 1).zfill(len(str(config[\"num_epochs\"])))\n",
    "\n",
    "            model_path = (\n",
    "                f\"./models/{net._name}/{net._name}_{lr_str}_{checkpoint_num}.pt\"\n",
    "            )\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print(f\"{config['num_epochs']} epochs took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    if config[\"log_training\"]:\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loader = DataLoader(Q3Data(0), batch_size=8, num_workers=4, shuffle=True)\n",
    "image_batch, labels = next(iter(tmp_loader))\n",
    "fig, ax_arr = plt.subplots(2, 4)\n",
    "for i in range(8):\n",
    "    img = image_batch[i].numpy()\n",
    "    ax_arr[i // 4, i % 4].imshow(img.reshape([28, 28]), cmap=\"gray\")\n",
    "    # ax_arr[i // 4, i % 4].axis(\"off\")\n",
    "    ax_arr[i // 4, i % 4].axes.get_yaxis().set_visible(False)\n",
    "    ax_arr[i // 4, i % 4].set_xlabel(labels[i].item())\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "# plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CustomMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 128)\n",
    "        self.fc5 = nn.Linear(128, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CustomMLP().to(device)\n",
    "model_config = {\n",
    "    \"model\": net,\n",
    "    \"lr\": 2e-4,\n",
    "    \"lr_variable\": False,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = train_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(log, model_config, select=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomMLP().to(device)\n",
    "model.eval()\n",
    "\n",
    "# load models 040 and 290\n",
    "model_path = f\"models/{model._name}/{model._name}_FixedLR_040.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q3Data(1)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# unused since not calculating loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
