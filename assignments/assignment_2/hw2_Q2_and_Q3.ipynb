{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 4 * torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Q2Data Class for custom Dataset\n",
    "\n",
    "the following class reads the data for Q2 and creates a torch dataset object for it. With this, you can easily \n",
    "use a dataloader to train your model. \n",
    "\n",
    "Make sure that the file \"hw2_Q2_data.npz\" is located properly (in this example, it should be in the same folder as this notebook.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q2Data(Dataset):\n",
    "    def __init__(self, mode=\"\", ray_tune=False):\n",
    "        # Ray Tune requires an absolute path\n",
    "        # go back 2 folders since ray goes 2 deeper\n",
    "        actual_cwd = str(Path.cwd().parents[1])\n",
    "        if not ray_tune:\n",
    "            actual_cwd = \".\"\n",
    "\n",
    "        data = np.load(f\"{actual_cwd}/data/hw2_Q2_and_Q3_data.npz\")\n",
    "        if \"train\" in mode:\n",
    "            # trainloader\n",
    "            self.images = data[\"arr_0\"].T\n",
    "            self.labels = data[\"arr_1\"]\n",
    "        elif \"val\" in mode:\n",
    "            # valloader\n",
    "            self.images = data[\"arr_2\"].T\n",
    "            self.labels = data[\"arr_3\"]\n",
    "        elif \"test\" in mode:\n",
    "            # testloader\n",
    "            self.images = data[\"arr_4\"].T\n",
    "            self.labels = data[\"arr_5\"]\n",
    "\n",
    "        self.images = np.float32(self.images) / 255.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.images[idx, :]\n",
    "        labels = self.labels[idx]\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on how to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 100\n",
    "n_workers = 4 * torch.cuda.device_count()\n",
    "\n",
    "train_data = Q2Data(\"train\")\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=b_size, num_workers=n_workers, shuffle=True\n",
    ")\n",
    "val_data = Q2Data(\"val\")\n",
    "# default shuffle=False\n",
    "val_loader = DataLoader(\n",
    "    val_data, batch_size=b_size, num_workers=n_workers, shuffle=True\n",
    ")\n",
    "test_data = Q2Data(\"test\")\n",
    "# default shuffle=False\n",
    "test_loader = DataLoader(\n",
    "    test_data, batch_size=b_size, num_workers=n_workers, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loader = DataLoader(Q2Data(\"train\"), batch_size=8, num_workers=4, shuffle=True)\n",
    "image_batch, labels = next(iter(tmp_loader))\n",
    "fig, ax_arr = plt.subplots(2, 4)\n",
    "for i in range(8):\n",
    "    img = image_batch[i].numpy()\n",
    "    ax_arr[i // 4, i % 4].imshow(img.reshape([28, 28]), cmap=\"gray\")\n",
    "    ax_arr[i // 4, i % 4].axis(\"off\")\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, dataloader_obj, loss_fn):\n",
    "    \"\"\"Function to easily test model on specified dataset\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        batch_loss = 0.0\n",
    "        batch_steps = 0\n",
    "        correct_pred = 0\n",
    "        total_pred = 0\n",
    "        for batch_id, (data, label) in enumerate(dataloader_obj):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            batch_loss += loss_fn(output, label).item()\n",
    "            batch_steps += 1\n",
    "\n",
    "            # indices where probability is maximum\n",
    "            _, val_pred = torch.max(output, 1)\n",
    "\n",
    "            correct_pred += (val_pred == label).sum().item()\n",
    "            total_pred += label.shape[0]\n",
    "\n",
    "        acc = correct_pred / total_pred\n",
    "        avg_loss = batch_loss / batch_steps  # average loss across batch\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"The original training function has been modified in order to use Ray's Tune\"\"\"\n",
    "\n",
    "    logger = {\n",
    "        \"train_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"acc\": np.zeros(config[\"num_epochs\"]),\n",
    "    }\n",
    "\n",
    "    #### LOAD DATA ####\n",
    "    ray_tune = config[\"ray_tune_enabled\"]\n",
    "    b_size = config[\"batch_size\"]\n",
    "    n_workers = 4 * torch.cuda.device_count()\n",
    "\n",
    "    train_data = Q2Data(\"train\", ray_tune)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    val_data = Q2Data(\"val\", ray_tune)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    #### INSTANTIATE MODEL ####\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # net = ShallowMLP().to(device)\n",
    "    net = DeepMLP().to(device)\n",
    "    # net = DeepWideMLP().to(device)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"]\n",
    "    )\n",
    "    if config[\"lr_variable\"]:\n",
    "        # what approximate epoch does convergence occur?\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[60], gamma=0.1)\n",
    "\n",
    "    #### BEGIN TRAINING ####\n",
    "    start_time = time.time()\n",
    "    for j in range(config[\"num_epochs\"]):\n",
    "        ## START OF BATCH ##\n",
    "        train_loss = 0.0\n",
    "        train_steps = 0\n",
    "        for batch_id, (data, label) in enumerate(train_dataloader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "\n",
    "            loss = loss_function(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        ## END OF BATCH ##\n",
    "        train_loss /= train_steps\n",
    "\n",
    "        # test model on validation dataset\n",
    "        val_loss, val_acc = test_model(net, val_dataloader, loss_function)\n",
    "\n",
    "        # send current training result back to Tune\n",
    "        if config[\"ray_tune_enabled\"]:\n",
    "            tune.report(loss=(train_loss), accuracy=(val_acc))\n",
    "\n",
    "        logger[\"train_loss\"][j] = train_loss\n",
    "        logger[\"val_loss\"][j] = val_loss\n",
    "        logger[\"acc\"][j] = val_acc\n",
    "\n",
    "        if config[\"log_training\"] and (j + 1) % config[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                f\"Epoch:{j+1}/{config['num_epochs']} \\\n",
    "                Train Loss: {logger['train_loss'][j]:.6f} \\\n",
    "                Val Loss: {logger['val_loss'][j]:.6f} \\\n",
    "                Acc: {logger['acc'][j]:.6f}\"\n",
    "            )\n",
    "\n",
    "        # make sure folder is created to place saved checkpoints\n",
    "        path = Path.cwd() / \"models\" / net._name\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        if config[\"save_model\"] and (j + 1) % config[\"save_interval\"] == 0:\n",
    "            checkpoint_num = str(j + 1).zfill(len(str(config[\"num_epochs\"])))\n",
    "            if config[\"lr_variable\"]:\n",
    "                lr_str = \"VarLR\"\n",
    "            else:\n",
    "                lr_str = \"FixedLR\"\n",
    "            model_path = (\n",
    "                f\"./models/{net._name}/{net._name}_{lr_str}_{checkpoint_num}.pt\"\n",
    "            )\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "\n",
    "        # this is used only to vary learning rate during training\n",
    "        if config[\"lr_variable\"]:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f\"{config['num_epochs']} epochs took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    if config[\"log_training\"]:\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FixedLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/ShallowMLP  # create folder for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for ideal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False  # remove to make cell work\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"lr_variable\": False,\n",
    "    \"batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "    \"log_training\": False,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"ray_tune_enabled\": True,\n",
    "}\n",
    "# enable early stopping\n",
    "asha_scheduler = ASHAScheduler(max_t=1000, grace_period=50)\n",
    "# number of samples to run\n",
    "n_samples = 20\n",
    "# run training with Tune\n",
    "analysis = tune.run(\n",
    "    train_model,\n",
    "    num_samples=n_samples,\n",
    "    config=search_space,\n",
    "    resources_per_trial={\"gpu\": 1},\n",
    "    scheduler=asha_scheduler,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    local_dir=\"./\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final model config for our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "log = train_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot train/val loss, and val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "epochs = model_config.get(\"num_epochs\")\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs)\n",
    "ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "ax.plot(x_axis, log.get(\"val_loss\"), label=\"Validation Loss\")\n",
    "ax.plot(x_axis, log.get(\"acc\"), label=\"Validation Accuracy\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.legend(loc=\"best\", prop={\"size\": 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 4 * torch.cuda.device_count()\n",
    "\n",
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "\n",
    "model = ShallowMLP().to(device)\n",
    "model.eval()\n",
    "model_path = f\"models/ShallowMLP/ShallowMLP_FixedLR_060.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multistep Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": True,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "log = train_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot train/val loss, and val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "epochs = model_config.get(\"num_epochs\")\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs)\n",
    "ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "ax.plot(x_axis, log.get(\"val_loss\"), label=\"Validation Loss\")\n",
    "ax.plot(x_axis, log.get(\"acc\"), label=\"Validation Accuracy\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.legend(loc=\"best\", prop={\"size\": 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 4 * torch.cuda.device_count()\n",
    "\n",
    "model = ShallowMLP().to(device)\n",
    "model.eval()\n",
    "model_path = f\"models/ShallowMLP/ShallowMLP_VarLR_060.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating accuracy of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = model_config[\"num_epochs\"]\n",
    "n_models = n_epochs // model_config[\"save_interval\"]\n",
    "\n",
    "model = DeepWideMLP().to(device)\n",
    "# created just for test_model()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "for k in range(n_models):\n",
    "    model_num = str((k + 1) * 10).zfill(len(str(n_epochs)))\n",
    "    model_path = f\"models/{model._name}/{model._name}_FixedLR_{model_num}.pt\"  # model_path = f\"models/ShallowMLP/ShallowMLP_FixedLR_{model_num}.pt\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    _, acc = test_model(model, test_dataloader, loss_func)\n",
    "\n",
    "    print(f\"{model_path}, Acc {acc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/DeepMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "log = train_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot train/val loss, and val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "epochs = model_config.get(\"num_epochs\")\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs)\n",
    "ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "ax.plot(x_axis, log.get(\"val_loss\"), label=\"Validation Loss\")\n",
    "ax.plot(x_axis, log.get(\"acc\"), label=\"Validation Accuracy\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.legend(loc=\"best\", prop={\"size\": 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "\n",
    "model = DeepMLP().to(device)\n",
    "model.eval()\n",
    "model_path = f\"models/DeepMLP/DeepMLP_FixedLR_080.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWideMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWideMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepWideMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "log = train_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot train/val loss, and val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "epochs = model_config.get(\"num_epochs\")\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs)\n",
    "ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "ax.plot(x_axis, log.get(\"val_loss\"), label=\"Validation Loss\")\n",
    "ax.plot(x_axis, log.get(\"acc\"), label=\"Validation Accuracy\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.legend(loc=\"best\", prop={\"size\": 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}\n",
    "\n",
    "model = DeepWideMLP().to(device)\n",
    "model.eval()\n",
    "model_path = f\"models/DeepWideMLP/DeepWideMLP_FixedLR_080.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "summary(DeepWideMLP().to(device), (batch_size, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(DeepMLP().to(device), (batch_size, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
