{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from carbontracker.tracker import CarbonTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 4 * torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Q2Data Class for custom Dataset\n",
    "\n",
    "the following class reads the data for Q2 and creates a torch dataset object for it. With this, you can easily \n",
    "use a dataloader to train your model. \n",
    "\n",
    "Make sure that the file \"hw2_Q2_data.npz\" is located properly (in this example, it should be in the same folder as this notebook.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q2Data(Dataset):\n",
    "    def __init__(self, mode=\"\", ray_tune=False):\n",
    "        # Ray Tune requires an absolute path\n",
    "        # go back 2 folders since ray goes 2 deeper\n",
    "        actual_cwd = str(Path.cwd().parents[1])\n",
    "        if not ray_tune:\n",
    "            actual_cwd = \".\"\n",
    "\n",
    "        data = np.load(f\"{actual_cwd}/data/hw2_Q2_and_Q3_data.npz\")\n",
    "        if \"train\" in mode:\n",
    "            # trainloader\n",
    "            self.images = data[\"arr_0\"].T\n",
    "            self.labels = data[\"arr_1\"]\n",
    "        elif \"val\" in mode:\n",
    "            # valloader\n",
    "            self.images = data[\"arr_2\"].T\n",
    "            self.labels = data[\"arr_3\"]\n",
    "        elif \"test\" in mode:\n",
    "            # testloader\n",
    "            self.images = data[\"arr_4\"].T\n",
    "            self.labels = data[\"arr_5\"]\n",
    "\n",
    "        self.images = np.float32(self.images) / 255.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.images[idx, :]\n",
    "        labels = self.labels[idx]\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on how to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 100\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    Q2Data(\"train\"), batch_size=b_size, num_workers=n_workers, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    Q2Data(\"val\"), batch_size=b_size, num_workers=n_workers, shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Q2Data(\"test\"), batch_size=b_size, num_workers=n_workers, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loader = DataLoader(Q2Data(\"train\"), batch_size=8, num_workers=4, shuffle=True)\n",
    "image_batch, labels = next(iter(tmp_loader))\n",
    "fig, ax_arr = plt.subplots(2, 4)\n",
    "for i in range(8):\n",
    "    img = image_batch[i].numpy()\n",
    "    ax_arr[i // 4, i % 4].imshow(img.reshape([28, 28]), cmap=\"gray\")\n",
    "    # ax_arr[i // 4, i % 4].axis(\"off\")\n",
    "    ax_arr[i // 4, i % 4].axes.get_yaxis().set_visible(False)\n",
    "    ax_arr[i // 4, i % 4].set_xlabel(labels[i].item())\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "# plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This was mainly how to use torchvision's make_grid()\n",
    "# # (8, 784) -> (8,1,28,28)\n",
    "# img = image_batch.reshape(-1, 28, 28).unsqueeze(1)\n",
    "# out = torchvision.utils.make_grid(img, nrow=4)\n",
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# ax.imshow(out.permute(1, 2, 0), interpolation=\"nearest\", aspect=\"auto\")\n",
    "# ax.axis(\"off\")\n",
    "# # plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log(log, model_config, select=False, save=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    if select:\n",
    "        # find min/max for criteria\n",
    "        selected = {}\n",
    "        for key in log:\n",
    "            if \"loss\" in key:\n",
    "                idx = np.argmin(log[key][9::10])\n",
    "                label = \"Min \"\n",
    "            elif \"acc\" in key:\n",
    "                idx = np.argmax(log[key][9::10])\n",
    "                label = \"Max \"\n",
    "            # 10 - 2000 epochs would be saved as 9 - 1999\n",
    "            # take every 10th epoch, determine a usable model\n",
    "            actual_idx = (idx + 1) * model_config[\"save_interval\"] - 1\n",
    "            selected[key] = actual_idx\n",
    "\n",
    "            print(key, actual_idx)\n",
    "            print(actual_idx, log.get(key)[actual_idx])\n",
    "\n",
    "            ax.plot(\n",
    "                actual_idx,\n",
    "                log.get(key)[actual_idx],\n",
    "                label=label + key,\n",
    "                markersize=16,\n",
    "                marker=\"X\",\n",
    "            )\n",
    "\n",
    "    epochs = model_config.get(\"num_epochs\")\n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "\n",
    "    ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "    ax.plot(x_axis, log.get(\"val_loss\"), label=\"Validation Loss\")\n",
    "    ax.plot(x_axis, log.get(\"acc\"), label=\"Validation Accuracy\")\n",
    "\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(16)\n",
    "\n",
    "    ax.legend(loc=\"best\", prop={\"size\": 12})\n",
    "    if save:\n",
    "        plt.savefig(f\"./LR_{model_config['lr']}_{model_config['num_epochs']}.jpg\")\n",
    "    plt.show()\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, data_generator, loss_fn):\n",
    "    \"\"\"Function to easily test model on specified dataset\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_loss, batch_steps = 0.0, 0\n",
    "        correct_pred, total_pred = 0, 0\n",
    "\n",
    "        for batch_id, (data, label) in enumerate(data_generator):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            batch_loss += loss_fn(output, label).item()\n",
    "            batch_steps += 1\n",
    "\n",
    "            # indices where probability is maximum\n",
    "            _, val_pred = torch.max(output, 1)\n",
    "\n",
    "            correct_pred += (val_pred == label).sum().item()\n",
    "            total_pred += label.shape[0]\n",
    "\n",
    "        # average loss/acc across ALL batches\n",
    "        # i.e. ACROSS specified dataset\n",
    "        avg_loss = batch_loss / batch_steps\n",
    "        avg_acc = correct_pred / total_pred\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"The original training function has been modified in order to use Ray's Tune\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_workers = 4 * torch.cuda.device_count()\n",
    "\n",
    "    logger = {\n",
    "        \"train_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"acc\": np.zeros(config[\"num_epochs\"]),\n",
    "    }\n",
    "\n",
    "    #### LOAD DATA ####\n",
    "    ray_tune = config[\"ray_tune_enabled\"]\n",
    "    b_size = config[\"batch_size\"]\n",
    "\n",
    "    train_data = Q2Data(\"train\", ray_tune)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    val_data = Q2Data(\"val\", ray_tune)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    tracker = CarbonTracker(epochs=config[\"num_epochs\"])\n",
    "\n",
    "    #### INSTANTIATE MODEL ####\n",
    "    net = config[\"model\"].to(device)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        net.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    if config[\"lr_variable\"]:\n",
    "        # what approximate epoch does convergence occur? 80 in this case\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1)\n",
    "\n",
    "    #### BEGIN TRAINING ####\n",
    "    start_time = time.time()\n",
    "    for j in range(config[\"num_epochs\"]):\n",
    "        tracker.epoch_start()\n",
    "        ## START OF BATCH ##\n",
    "        train_loss, train_steps = 0.0, 0\n",
    "        for batch_id, (data, label) in enumerate(train_dataloader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "\n",
    "            loss = loss_function(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        ## END OF BATCH ##\n",
    "\n",
    "        # average training loss for 1 epoch\n",
    "        train_loss /= train_steps\n",
    "\n",
    "        # test model on validation dataset\n",
    "        val_loss, val_acc = test_model(net, val_dataloader, loss_function)\n",
    "\n",
    "        # send current training result back to Tune\n",
    "        if config[\"ray_tune_enabled\"]:\n",
    "            tune.report(loss=(train_loss), accuracy=(val_acc))\n",
    "\n",
    "        logger[\"train_loss\"][j] = train_loss\n",
    "        logger[\"val_loss\"][j] = val_loss\n",
    "        logger[\"acc\"][j] = val_acc\n",
    "\n",
    "        if config[\"log_training\"] and (j + 1) % config[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                f\"Epoch:{j+1}/{config['num_epochs']} \\\n",
    "                Train Loss: {logger['train_loss'][j]:.6f} \\\n",
    "                Val Loss: {logger['val_loss'][j]:.6f} \\\n",
    "                Acc: {logger['acc'][j]:.6f}\"\n",
    "            )\n",
    "\n",
    "        # make sure folder is created to place saved checkpoints\n",
    "        path = Path.cwd() / \"models\" / net._name\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        if config[\"save_model\"] and (j + 1) % config[\"save_interval\"] == 0:\n",
    "            lr_str = \"VarLR\" if config[\"lr_variable\"] else \"FixedLR\"\n",
    "            # pad with appropriate number of zeros i.e. epoch 10 named as 010\n",
    "            checkpoint_num = str(j + 1).zfill(len(str(config[\"num_epochs\"])))\n",
    "\n",
    "            model_path = (\n",
    "                f\"./models/{net._name}/{net._name}_{lr_str}_{checkpoint_num}.pt\"\n",
    "            )\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "\n",
    "        # this is used only to vary learning rate during training\n",
    "        if config[\"lr_variable\"]:\n",
    "            scheduler.step()\n",
    "\n",
    "        tracker.epoch_end()\n",
    "    # Optional: Add a stop in case of early termination before all monitor_epochs has\n",
    "    # been monitored to ensure that actual consumption is reported.\n",
    "    tracker.stop()\n",
    "\n",
    "    print(f\"{config['num_epochs']} epochs took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    if config[\"log_training\"]:\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/ShallowMLP  # create folder for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCHING FOR IDEAL PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False  # remove to make cell work\n",
    "# search_space = {\n",
    "#     \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "#     \"lr_variable\": False,\n",
    "#     \"momentum\": 0.9,\n",
    "#     \"weight_decay\": 1e-4,\n",
    "#     \"batch_size\": tune.choice([64, 128, 256]),\n",
    "#     \"log_training\": False,\n",
    "#     \"log_interval\": 10,\n",
    "#     \"save_model\": False,\n",
    "#     \"save_interval\": 10,\n",
    "#     \"num_epochs\": 500,\n",
    "#     \"ray_tune_enabled\": True,\n",
    "# }\n",
    "\n",
    "# # enable early stopping\n",
    "# asha_scheduler = ASHAScheduler(max_t=500, grace_period=50)\n",
    "# # number of samples to run\n",
    "# n_samples = 50\n",
    "# # run training with Tune\n",
    "# analysis = tune.run(\n",
    "#     train_model,\n",
    "#     num_samples=n_samples,\n",
    "#     config=search_space,\n",
    "#     resources_per_trial={\"gpu\": 1},\n",
    "#     scheduler=asha_scheduler,\n",
    "#     metric=\"accuracy\",\n",
    "#     mode=\"max\",\n",
    "#     local_dir=\"./\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linspaced LRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_list = np.linspace(1,50,50) * 1e-3\n",
    "# summary = []\n",
    "# for cur_lr in lr_list:\n",
    "#     # create new instance for every lr change\n",
    "#     net = ShallowMLP().to(device)\n",
    "#     model_config = {\n",
    "#         \"model\": net,\n",
    "#         \"lr\": cur_lr,\n",
    "#         \"lr_variable\": False,\n",
    "#         \"momentum\": 0.9,\n",
    "#         \"weight_decay\": 1e-4,\n",
    "#         \"batch_size\": 128,\n",
    "#         \"log_training\": True,\n",
    "#         \"log_interval\": 10,\n",
    "#         \"save_model\": False,\n",
    "#         \"save_interval\": 10,\n",
    "#         \"num_epochs\": 300,\n",
    "#         \"ray_tune_enabled\": False,\n",
    "#     }\n",
    "#     log = train_model(model_config)\n",
    "#     selected= plot_log(log, model_config, select=True, save=True)\n",
    "#     summary.append(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ShallowMLP FixedLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ShallowMLP().to(device)\n",
    "model_config = {\n",
    "    \"model\": net,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 1500,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = train_model(model_config)\n",
    "# plot_log(log, model_config, select=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowMLP().to(device)\n",
    "model.eval()\n",
    "# select 210, 240, 1490 models\n",
    "model_path = f\"models/{model._name}/{model._name}_FixedLR_1490.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable (Multistep) Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ShallowMLP().to(device)\n",
    "model_config = {\n",
    "    \"model\": net,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": True,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 1500,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = train_model(model_config)\n",
    "# plot_log(log, model_config, select=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowMLP().to(device)\n",
    "model.eval()\n",
    "\n",
    "# load 1270, 1430 and 1500 models\n",
    "model_path = f\"models/{model._name}/{model._name}_VarLR_1500.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/DeepMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepMLP().to(device)\n",
    "model_config = {\n",
    "    \"model\": net,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": True,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 1500,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = train_model(model_config)\n",
    "# _ = plot_log(log, model_config, select=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_log(log, model_config, select=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepMLP().to(device)\n",
    "model.eval()\n",
    "\n",
    "# load 1240 model\n",
    "model_path = f\"models/{model._name}/{model._name}_VarLR_1240.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWideMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWideMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepWideMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepWideMLP().to(device)\n",
    "model_config = {\n",
    "    \"model\": net,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_variable\": True,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"num_epochs\": 1500,\n",
    "    \"ray_tune_enabled\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = train_model(model_config)\n",
    "# _ = plot_log(log, model_config, select=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepWideMLP().to(device)\n",
    "model.eval()\n",
    "\n",
    "# load 1010 model\n",
    "model_path = f\"models/{model._name}/{model._name}_VarLR_1010.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_data = Q2Data(\"test\", ray_tune=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_config.get(\"batch_size\"),\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# arbitrary loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "_, acc = test_model(model, test_dataloader, loss_func)\n",
    "print(f\"Accuracy on test dataset: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, _ = next(iter(tmp_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_b_size = 1\n",
    "summary(DeepMLP().to(device), (tmp_b_size, image_batch.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_b_size = 1\n",
    "summary(DeepWideMLP().to(device), (tmp_b_size, image_batch.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
