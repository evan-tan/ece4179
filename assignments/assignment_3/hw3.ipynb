{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Example\n",
    "\n",
    "the following class reads the data for the third assignment and creates a torch dataset object for it. With this, you can easily use a dataloader to train your model. \n",
    "\n",
    "Due to size limit on moodle, the data for this assignment should be obtained from \n",
    "\n",
    "https://drive.google.com/file/d/1khzPamThzWScipEfMmOPevtfWV7Tx6UL/view?usp=sharing\n",
    "\n",
    "\n",
    "Make sure that the file \"hw3.npz\" is located properly (in this example, it should be in the same folder as this notebook).\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STLData(Dataset):\n",
    "    def __init__(self, mode=\"\", transform=None):\n",
    "        data = np.load(\"hw3.npz\")\n",
    "        if \"train\" in mode:\n",
    "            # trainloader\n",
    "            self.images = data[\"arr_0\"]\n",
    "            self.labels = data[\"arr_1\"]\n",
    "        elif \"val\" in mode:\n",
    "            # valloader\n",
    "            self.images = data[\"arr_2\"]\n",
    "            self.labels = data[\"arr_3\"]\n",
    "        elif \"test\" in mode:\n",
    "            # testloader\n",
    "            self.images = data[\"arr_4\"]\n",
    "            self.labels = data[\"arr_5\"]\n",
    "        else:\n",
    "            raise ValueError(\"mode should be 'train', 'val' or 'test'\")\n",
    "\n",
    "        self.images = np.float32(self.images) / 1.0\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.images[idx, :]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how you can create a dataloader. \n",
    "First read the data. Note that the STL10 class can work with torchvision.transforms that are required in HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modified STLData class\n",
    "train_set = STLData(\"train\", T.ToTensor())\n",
    "val_set = STLData(\"val\", T.ToTensor())\n",
    "test_set = STLData(\"test\", T.ToTensor())\n",
    "batch_size = 100\n",
    "n_workers = 0 * multiprocessing.cpu_count()\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=n_workers\n",
    ")\n",
    "image_batch, labels = next(iter(trainloader))\n",
    "\n",
    "fig, ax_arr = plt.subplots(2, 4)\n",
    "for i in range(8):\n",
    "    img = (image_batch[i] / 255.0).permute(1, 2, 0)\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    ax_arr[row, col].imshow(img)\n",
    "    # ax_arr[i // 4, i % 4].axis(\"off\")\n",
    "    ax_arr[row, col].axes.get_yaxis().set_visible(False)\n",
    "    ax_arr[row, col].set_xlabel(labels[i].item())\n",
    "    ax_arr[row, col].set_xticklabels([])\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a batchsize of 100, you can have a dataloader as follows for your training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = STLData(\"train\", T.ToTensor())\n",
    "val_set = STLData(\"val\", T.ToTensor())\n",
    "test_set = STLData(\"test\", T.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_log(log, model_config, save=False, select=True):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figheight(7.5)\n",
    "    fig.set_figwidth(12)\n",
    "    # use ax1 for loss, ax2 for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    epochs = model_config.get(\"num_epochs\")\n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "    color = iter(cm.rainbow(np.linspace(0, 1, len(log))))\n",
    "    # storage for all max/min values based on keys\n",
    "    selected = dict.fromkeys(log)\n",
    "\n",
    "    count = 0\n",
    "    for key, values in log.items():\n",
    "        c = next(color)\n",
    "        key_str = key.replace(\"_\", \" \").title()\n",
    "        # plot data\n",
    "        if \"loss\" in key:\n",
    "            ax1.plot(x_axis, values, color=c, label=key_str)\n",
    "        elif \"acc\" in key:\n",
    "            ax2.plot(x_axis, values, color=c, label=key_str)\n",
    "        if select:\n",
    "            if \"loss\" in key:\n",
    "                # search for min\n",
    "                x = np.argmin(values) + 1\n",
    "                y = np.amin(values)\n",
    "                ax1.plot(\n",
    "                    x,\n",
    "                    y,\n",
    "                    color=c,\n",
    "                    label=f\"Min. {key}\",\n",
    "                    markersize=16,\n",
    "                    marker=\"x\",\n",
    "                )\n",
    "            elif \"acc\" in key:\n",
    "                # search for max\n",
    "                x = np.argmax(log[key]) + 1\n",
    "                y = np.amax(log[key])\n",
    "                ax2.plot(\n",
    "                    x,\n",
    "                    y,\n",
    "                    color=c,\n",
    "                    label=f\"Max. {key}\",\n",
    "                    markersize=16,\n",
    "                    marker=\"x\",\n",
    "                )\n",
    "            # save values in dict\n",
    "            # format: (epoch id, data value)\n",
    "            selected[key] = (x, y)\n",
    "        count += 1\n",
    "\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_xlabel(\"Number of Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "    # 0 = 'best', 7 = 'center right'\n",
    "    fig.legend(loc=7, bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f\"./LR_{model_config['lr']}_{model_config['num_epochs']}.jpg\")\n",
    "\n",
    "    plt.title(\n",
    "        f\"{model_cfg['model']._name} Learning Rate={str(model_cfg['lr'])} Batch Size={(model_cfg['batch_size'])} Max Val Acc={selected['val_acc'][1]} @ Epoch {selected['val_acc'][0]}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    if select:\n",
    "        return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(net, data_generator, loss_fn, transform=None):\n",
    "    \"\"\"Function to easily test model on specified dataset\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_loss, batch_steps = 0.0, 0\n",
    "        correct_pred, total_pred = 0, 0\n",
    "\n",
    "        for batch_id, (data, label) in enumerate(data_generator):\n",
    "            if transform is not None:\n",
    "                data = transform(data.cuda())\n",
    "            data, label = data.to(device), label.long().to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            batch_loss += loss_fn(output, label).item()\n",
    "            batch_steps += 1\n",
    "\n",
    "            # indices where probability is maximum\n",
    "            _, pred_label = torch.max(output, 1)\n",
    "            correct_pred += (pred_label == label).sum().item()\n",
    "            total_pred += label.shape[0]\n",
    "\n",
    "        # average loss/acc across ALL batches\n",
    "        # i.e. ACROSS specified dataset\n",
    "        avg_loss = batch_loss / batch_steps\n",
    "        avg_acc = correct_pred / total_pred\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_workers = 0 * torch.cuda.device_count()\n",
    "\n",
    "    logger = {\n",
    "        \"train_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"train_acc\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_acc\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"test_acc\": np.zeros(config[\"num_epochs\"]),\n",
    "    }\n",
    "\n",
    "    #### LOAD DATA ####\n",
    "    b_size = config[\"batch_size\"]\n",
    "\n",
    "    train_transform = config.get(\"train_transform\")\n",
    "    val_transform = config.get(\"val_transform\")\n",
    "    test_transform = config.get(\"test_transform\")\n",
    "\n",
    "    train_data = STLData(\"train\", T.ToTensor())\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    val_data = STLData(\"val\", T.ToTensor())\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    test_data = STLData(\"test\", T.ToTensor())\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    #### INSTANTIATE MODEL ####\n",
    "    net = config[\"model\"].to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    if \"Adam\" in config[\"optimizer\"]:\n",
    "        optimizer = optim.Adam(\n",
    "            net.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "    elif \"SGD\" in config[\"optimizer\"]:\n",
    "        optimizer = optim.SGD(\n",
    "            net.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            momentum=config[\"momentum\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if config[\"lr_scheduler\"]:\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \"max\", 0.5, patience=config[\"num_epochs\"] // 10, verbose=True\n",
    "        )\n",
    "\n",
    "    #### BEGIN TRAINING ####\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    for j in range(config[\"num_epochs\"]):\n",
    "        ## START OF EPOCH ##\n",
    "        train_loss, train_steps = 0.0, 0\n",
    "        net.train()\n",
    "        for batch_id, (data, label) in enumerate(train_dataloader):\n",
    "            if train_transform is not None:\n",
    "                data = train_transform(data.cuda())\n",
    "            data, label = data.to(device), label.long().to(device)\n",
    "\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = net(data)\n",
    "                loss = loss_function(output, label)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        ## END OF EPOCH ##\n",
    "\n",
    "        # average training loss for 1 epoch\n",
    "        train_loss /= train_steps\n",
    "\n",
    "        # test model on validation dataset\n",
    "        _, train_acc = test_model(net, train_dataloader, loss_function, train_transform)\n",
    "        val_loss, val_acc = test_model(\n",
    "            net, val_dataloader, loss_function, val_transform\n",
    "        )\n",
    "        _, test_acc = test_model(net, test_dataloader, loss_function, test_transform)\n",
    "\n",
    "        if config[\"lr_scheduler\"]:\n",
    "            scheduler.step(val_acc)\n",
    "\n",
    "        logger[\"train_loss\"][j] = train_loss\n",
    "        logger[\"val_loss\"][j] = val_loss\n",
    "        logger[\"train_acc\"][j] = train_acc\n",
    "        logger[\"val_acc\"][j] = val_acc\n",
    "        logger[\"test_acc\"][j] = test_acc\n",
    "\n",
    "        if config[\"log_training\"] and (j + 1) % config[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                f\"Epoch:{j+1}/{config['num_epochs']}\",\n",
    "                f\"Train Loss: {logger['train_loss'][j]:.4f}\",\n",
    "                f\"Train Acc: {logger['train_acc'][j]:.4f}\",\n",
    "                f\"Val Loss: {logger['val_loss'][j]:.4f}\",\n",
    "                f\"Val Acc: {logger['val_acc'][j]:.4f}\",\n",
    "                f\"Test Acc: {logger['test_acc'][j]:.4f}\",\n",
    "            )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if config[\"save_model\"]:\n",
    "                # make sure folder is created to place saved checkpoints\n",
    "                path = Path.cwd() / \"models\" / net._name\n",
    "                if not path.exists():\n",
    "                    path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "                # pad with appropriate number of zeros i.e. epoch 10 named as 010\n",
    "                checkpoint_num = str(j + 1).zfill(len(str(config[\"num_epochs\"])))\n",
    "                model_path = f\"./models/{net._name}/{net._name}_{checkpoint_num}.pt\"\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print(f\"{config['num_epochs']} epochs took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    if config[\"log_training\"]:\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ShallowCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShallowCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, (7, 7), stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(96, 64, (5, 5), stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3, 3), stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(1152, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shallow_net = ShallowCNN().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": shallow_net,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_scheduler\": False,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"num_epochs\": 60,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert False\n",
    "log_shallow = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log(log_shallow, model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shallow_net = ShallowCNN().to(device)\n",
    "shallow_net.eval()\n",
    "# select 30.pt\n",
    "model_path = f\"models/{shallow_net._name}/{shallow_net._name}_30.pt\"\n",
    "shallow_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    STLData(\"test\", T.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=n_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "_, test_acc = test_model(shallow_net, test_dataloader, nn.CrossEntropyLoss())\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def store_model_outputs(net, data_generator):\n",
    "    # # 3D list, where index = class label\n",
    "    # # each row of top level list, format: [prediction prob, True or False, image data]\n",
    "    # correct_bin = [[] for _ in range(10)]\n",
    "    # wrong_bin = [[] for _ in range(10)]\n",
    "\n",
    "    # let's try with tensors\n",
    "    images = []  # (2000,)\n",
    "    labels = []  # (2000,)\n",
    "    # probability from softmax\n",
    "    confidence = []  # (2000,10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        # loop through val dataset, collect all scores per class\n",
    "        for batch_id, (data, label) in enumerate(data_generator):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = net(data)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(output, dim=1)\n",
    "\n",
    "            images += data\n",
    "            labels += label\n",
    "            confidence += probs\n",
    "            # VERY CONVOLUTED WAY\n",
    "            # # output True/False for each batch\n",
    "            # all_idx = torch.argmax(output, 1) == label\n",
    "            # for idx, val in enumerate(all_idx):\n",
    "            #     label_ = label[idx]\n",
    "            #     # store in correct_bin\n",
    "            #     if val == True:\n",
    "            #         correct_bin[label_].append([probs[idx].max(), all_idx[idx], data[idx]])\n",
    "            #     # store in wrong_bin\n",
    "            #     elif val == False:\n",
    "            #         wrong_bin[label_].append([probs[idx].max(), all_idx[idx], data[idx]])\n",
    "\n",
    "    # convert lists of tensors to single tensor and overwrite variables\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)  # torch.Tensor(labels) also works\n",
    "    confidence = torch.stack(confidence)\n",
    "\n",
    "    return labels, confidence, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_model_outputs(net, mode=\"\"):\n",
    "    \"\"\"Visualize model's top 5 images for each class on val dataset, defaults to correct predictions\"\"\"\n",
    "\n",
    "    val_data = STLData(mode=\"val\", transform=T.ToTensor())\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=model_cfg[\"batch_size\"],\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    labels, confidence, images = store_model_outputs(net, val_dataloader)\n",
    "    # find correct labels indices\n",
    "    idx = confidence.argmax(dim=1) == labels\n",
    "    if mode != \"correct\":\n",
    "        idx = ~idx  # jank bitwise complement\n",
    "\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "    confidence = confidence[idx]\n",
    "\n",
    "    display_img = []\n",
    "    n_img = 5\n",
    "    for j in range(confidence.shape[1]):\n",
    "        top_n_idx = torch.argsort(confidence[:, j], descending=True)[:n_img]\n",
    "        display_img += images[top_n_idx]\n",
    "    display_img = (torch.stack(display_img) / 255.0).cpu()\n",
    "    out = torchvision.utils.make_grid(display_img, nrow=n_img)\n",
    "    fig, ax = plt.subplots(figsize=(10, 20))\n",
    "    ax.imshow(out.permute(1, 2, 0), interpolation=\"nearest\", aspect=\"auto\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correct images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_model_outputs(shallow_net, \"correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrong images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_model_outputs(shallow_net, \"wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OLD NEWS DO NOT USE VERY BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def process_bin(data_bin):\n",
    "#     num_img = 5\n",
    "#     all_imgs = torch.empty(len(data_bin) * num_img, 3, 96, 96)\n",
    "#     for i, list_2d in enumerate(data_bin):\n",
    "#         # sort based on first element i.e. probabilities\n",
    "#         top_5 = sorted(list_2d, key=lambda x: x[0], reverse=True)[:num_img]\n",
    "#         top_5_tensor = torch.empty(num_img, *top_5[0][2].shape)\n",
    "#         for j in range(num_img):\n",
    "#             top_5_tensor[j] = top_5[j][2]\n",
    "#         offset = i * num_img\n",
    "#         all_imgs[0 + offset : 5 + offset] = top_5_tensor\n",
    "\n",
    "#     all_imgs = all_imgs / 255.0\n",
    "#     out = torchvision.utils.make_grid(all_imgs, nrow=num_img)\n",
    "#     fig, ax = plt.subplots(figsize=(10, 20))\n",
    "#     ax.imshow(out.permute(1, 2, 0), interpolation=\"nearest\", aspect=\"auto\")\n",
    "#     ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process_bin(correct_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process_bin(wrong_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # loop through different classes\n",
    "# fig, ax = plt.subplots(10, 5)\n",
    "# for i, list_2d in enumerate(correct_bin):\n",
    "#     # sort based on first element i.e. probabilities\n",
    "#     top_5 = sorted(list_2d, key=lambda x: x[0], reverse=True)[:5]\n",
    "#     for j in range(5):\n",
    "#         img = (top_5[j][2] / 255.0).permute(1, 2, 0).cpu()\n",
    "#         ax[i, j].imshow(img)\n",
    "#         ax[i, j].axes.get_yaxis().set_visible(False)\n",
    "#         ax[i, j].axes.get_xaxis().set_visible(False)\n",
    "# fig.set_figheight(10)\n",
    "# fig.set_figwidth(10)\n",
    "# plt.subplots_adjust(wspace=0.00, hspace=0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix(\n",
    "    net,\n",
    "    data_generator,\n",
    "    labels,\n",
    "):\n",
    "    \"\"\"Create confusion matrix based for a given dataset and respective labels\"\"\"\n",
    "    true_labels, confidence, _ = store_model_outputs(net, data_generator)\n",
    "    pred_labels = confidence.argmax(dim=1).cpu()\n",
    "    true_labels = true_labels.cpu()\n",
    "\n",
    "    cm = confusion_matrix(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "    )\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(11, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\"d\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    if type(data_generator) == DataLoader:\n",
    "        if len(data_generator.dataset) == len(STLData(\"test\")):\n",
    "            dataset = \"Test\"\n",
    "        elif len(data_generator.dataset) == len(STLData(\"train\")):\n",
    "            dataset = \"Train\"\n",
    "        elif len(data_generator.dataset) == len(STLData(\"val\")):\n",
    "            dataset = \"Validation\"\n",
    "    plt.title(f\"Confusion Matrix for {dataset} Dataset\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    \"planes\",\n",
    "    \"birds\",\n",
    "    \"cars\",\n",
    "    \"cats\",\n",
    "    \"deer\",\n",
    "    \"dogs\",\n",
    "    \"horses\",\n",
    "    \"monkeys\",\n",
    "    \"ships\",\n",
    "    \"large vehicles\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CM Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    STLData(mode=\"train\", transform=T.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=n_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "make_confusion_matrix(shallow_net, train_dataloader, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CM val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    STLData(mode=\"val\", transform=T.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=n_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "make_confusion_matrix(shallow_net, val_dataloader, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    STLData(mode=\"test\", transform=T.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=n_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "make_confusion_matrix(shallow_net, test_dataloader, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# DeepCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "\n",
    "        self.blocks = self._build_blocks()\n",
    "        # since the output of our conv blocks is (6,6)\n",
    "        self.gap = nn.AvgPool2d(kernel_size=6, stride=1)\n",
    "        self.fc1 = nn.Linear(192, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def _build_blocks(self):\n",
    "        conv_blk_dims = [3, 32, 64, 128, 192]\n",
    "        blocks_list = []\n",
    "        for i in range(len(conv_blk_dims) - 1):\n",
    "            conv_block = self._create_conv_block(conv_blk_dims[i], conv_blk_dims[i + 1])\n",
    "            named_block = (f\"Conv-Blk-{i+1}\", conv_block)\n",
    "            # blocks_list.append(conv_block)\n",
    "            blocks_list.append(named_block)\n",
    "        # return nn.Sequential(*blocks_list)\n",
    "        return nn.Sequential(OrderedDict(blocks_list))\n",
    "\n",
    "    def _create_conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create conv_block based on in/out channels\"\"\"\n",
    "        conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (3, 3), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, (1, 1), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        return conv_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze()\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DeepCNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## No transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_net = DeepCNN().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": deep_net,\n",
    "    \"lr\": 2.5e-4,\n",
    "    \"lr_scheduler\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"num_epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert False\n",
    "log_deep = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log(log_deep, model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_net = DeepCNN().to(device)\n",
    "deep_net.eval()\n",
    "# select 058.pt\n",
    "model_path = f\"./models/{deep_net._name}_058.pt\"\n",
    "deep_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    STLData(mode=\"test\", transform=T.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=n_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "_, test_acc = test_model(deep_net, test_dataloader, nn.CrossEntropyLoss())\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### calculate mean and std of our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mean_std(mode=\"\"):\n",
    "    assert mode == \"train\" or mode == \"val\" or mode == \"test\"\n",
    "    dataset = STLData(mode, T.ToTensor())\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=len(dataset),  # lmao\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    for _, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(\"cpu\")\n",
    "        data = data.permute(1, 0, 2, 3).flatten(start_dim=1) / 255.0\n",
    "        # mean across channels\n",
    "        mean = data.mean(1).tolist()\n",
    "        std = data.std(1).tolist()\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_mean, t_std = get_mean_std(\"train\")\n",
    "train_transform = nn.Sequential(\n",
    "    T.RandomRotation(degrees=45),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.05),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.Normalize(mean=t_mean, std=t_std),\n",
    ")\n",
    "#     T.RandomCrop(size=(96,96))\n",
    "#     T.RandomResizedCrop(size=96),\n",
    "#     T.ColorJitter(brightness=0.5, hue=0.3),\n",
    "#     T.RandomErasing(p=0.5, scale=(0.02, 0.2)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_net = DeepCNN().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": deep_net,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"momentum\": 0.9,  # only for SGD\n",
    "    \"weight_decay\": 0,\n",
    "    \"lr\": 5e-4,\n",
    "    \"lr_scheduler\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"num_epochs\": 300,\n",
    "    \"train_transform\": train_transform,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_deep_T = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log(log_deep_T, model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = DeepCNN().to(device)\n",
    "net.eval()\n",
    "# select 058.pt\n",
    "model_path = f\"./models/{net._name}/{net._name}_256.pt\"\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    STLData(mode=\"test\", transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=n_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "_, test_acc = test_model(net, test_dataloader, nn.CrossEntropyLoss())\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCNNv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepCNNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNNv2, self).__init__()\n",
    "\n",
    "        self.blocks = self._build_blocks()\n",
    "        # since the output of our conv blocks is (6,6)\n",
    "        self.gap = nn.AvgPool2d(kernel_size=6, stride=1)\n",
    "        self.fc1 = nn.Linear(512, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def _build_blocks(self):\n",
    "        conv_blk_dims = [3, 64, 128, 256, 512]\n",
    "        blocks_list = []\n",
    "        for i in range(len(conv_blk_dims) - 1):\n",
    "            conv_block = self._create_conv_block(conv_blk_dims[i], conv_blk_dims[i + 1])\n",
    "            named_block = (f\"Conv-Blk-{i+1}\", conv_block)\n",
    "            # blocks_list.append(conv_block)\n",
    "            blocks_list.append(named_block)\n",
    "        # return nn.Sequential(*blocks_list)\n",
    "        return nn.Sequential(OrderedDict(blocks_list))\n",
    "\n",
    "    def _create_conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create conv_block based on in/out channels\"\"\"\n",
    "        conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (11, 11), stride=2, padding=5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "        return conv_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze()\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_netv2 = DeepCNNv2().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": deep_netv2,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"momentum\": 0.9,  # only for SGD\n",
    "    \"weight_decay\": 0,\n",
    "    \"lr\": 5e-4,\n",
    "    \"lr_scheduler\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"num_epochs\": 300,\n",
    "    \"train_transform\": train_transform,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_deep_Tv2 = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
