{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Example\n",
    "\n",
    "the following class reads the data for the third assignment and creates a torch dataset object for it. With this, you can easily use a dataloader to train your model. \n",
    "\n",
    "Due to size limit on moodle, the data for this assignment should be obtained from \n",
    "\n",
    "https://drive.google.com/file/d/1khzPamThzWScipEfMmOPevtfWV7Tx6UL/view?usp=sharing\n",
    "\n",
    "\n",
    "Make sure that the file \"hw3.npz\" is located properly (in this example, it should be in the same folder as this notebook).\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STLData(Dataset):\n",
    "    def __init__(self, mode=\"\", transform=None):\n",
    "        data = np.load(\"hw3.npz\")\n",
    "        if \"train\" in mode:\n",
    "            # trainloader\n",
    "            self.images = data[\"arr_0\"]\n",
    "            self.labels = data[\"arr_1\"]\n",
    "        elif \"val\" in mode:\n",
    "            # valloader\n",
    "            self.images = data[\"arr_2\"]\n",
    "            self.labels = data[\"arr_3\"]\n",
    "        elif \"test\" in mode:\n",
    "            # testloader\n",
    "            self.images = data[\"arr_4\"]\n",
    "            self.labels = data[\"arr_5\"]\n",
    "        else:\n",
    "            raise ValueError(\"mode should be 'train', 'val' or 'test'\")\n",
    "\n",
    "        self.images = np.float32(self.images) / 1.0\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.images[idx, :]\n",
    "        labels = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how you can create a dataloader. \n",
    "First read the data. Note that the STL10 class can work with torchvision.transforms that are required in HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_set = STLData(trn_val_tst=0, transform=torchvision.transforms.ToTensor())\n",
    "# val_set = STLData(trn_val_tst=1, transform=torchvision.transforms.ToTensor())\n",
    "# test_set = STLData(trn_val_tst=2, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# modified STLData class\n",
    "train_set = STLData(\"train\", transform=torchvision.transforms.ToTensor())\n",
    "val_set = STLData(\"val\", transform=torchvision.transforms.ToTensor())\n",
    "test_set = STLData(\"test\", transform=torchvision.transforms.ToTensor())\n",
    "batch_size = 100\n",
    "n_workers = multiprocessing.cpu_count()\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=n_workers\n",
    ")\n",
    "image_batch, labels = next(iter(trainloader))\n",
    "for tmpC1 in range(8):\n",
    "    img = np.moveaxis(image_batch[tmpC1].numpy(), 0, 2)\n",
    "    plt.subplot(2, 4, tmpC1 + 1)\n",
    "    plt.imshow(img / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a batchsize of 100, you can have a dataloader as follows for your training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = STLData(\"train\", transform=torchvision.transforms.ToTensor())\n",
    "val_set = STLData(\"val\", transform=torchvision.transforms.ToTensor())\n",
    "test_set = STLData(\"test\", transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_log(log, model_config, save=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = model_config.get(\"num_epochs\")\n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "\n",
    "    ax.plot(x_axis, log.get(\"train_loss\"), label=\"Train Loss\")\n",
    "    ax.plot(x_axis, log.get(\"val_loss\"), label=\"Validation Loss\")\n",
    "    ax.plot(x_axis, log.get(\"acc\"), label=\"Validation Accuracy\")\n",
    "\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(16)\n",
    "\n",
    "    ax.legend(loc=\"best\", prop={\"size\": 12})\n",
    "    if save:\n",
    "        plt.savefig(f\"./LR_{model_config['lr']}_{model_config['num_epochs']}.jpg\")\n",
    "    plt.show()\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(net, data_generator, loss_fn):\n",
    "    \"\"\"Function to easily test model on specified dataset\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_loss, batch_steps = 0.0, 0\n",
    "        correct_pred, total_pred = 0, 0\n",
    "\n",
    "        for batch_id, (data, label) in enumerate(data_generator):\n",
    "            label = label.long()\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            batch_loss += loss_fn(output, label).item()\n",
    "            batch_steps += 1\n",
    "\n",
    "            # indices where probability is maximum\n",
    "            _, pred_label = torch.max(output, 1)\n",
    "            correct_pred += (pred_label == label).sum().item()\n",
    "            total_pred += label.shape[0]\n",
    "\n",
    "        # average loss/acc across ALL batches\n",
    "        # i.e. ACROSS specified dataset\n",
    "        avg_loss = batch_loss / batch_steps\n",
    "        avg_acc = correct_pred / total_pred\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_workers = 0 * torch.cuda.device_count()\n",
    "\n",
    "    logger = {\n",
    "        \"train_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"acc\": np.zeros(config[\"num_epochs\"]),\n",
    "    }\n",
    "\n",
    "    #### LOAD DATA ####\n",
    "    b_size = config[\"batch_size\"]\n",
    "\n",
    "    train_data = STLData(mode=\"train\", transform=torchvision.transforms.ToTensor())\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_data = STLData(mode=\"val\", transform=torchvision.transforms.ToTensor())\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    #### INSTANTIATE MODEL ####\n",
    "    net = config[\"model\"].to(device)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"max\", 0.5, patience=config[\"num_epochs\"] // 10, verbose=True\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    #### BEGIN TRAINING ####\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    for j in range(config[\"num_epochs\"]):\n",
    "        ## START OF EPOCH ##\n",
    "        train_loss, train_steps = 0.0, 0\n",
    "        for batch_id, (data, label) in enumerate(train_dataloader):\n",
    "            label = label.long()\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = net(data)\n",
    "                loss = loss_function(output, label)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        ## END OF EPOCH ##\n",
    "\n",
    "        # average training loss for 1 epoch\n",
    "        train_loss /= train_steps\n",
    "\n",
    "        # test model on validation dataset\n",
    "        val_loss, val_acc = test_model(net, val_dataloader, loss_function)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        logger[\"train_loss\"][j] = train_loss\n",
    "        logger[\"val_loss\"][j] = val_loss\n",
    "        logger[\"acc\"][j] = val_acc\n",
    "\n",
    "        if config[\"log_training\"] and (j + 1) % config[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                f\"Epoch:{j+1}/{config['num_epochs']} \\\n",
    "                Train Loss: {logger['train_loss'][j]:.6f} \\\n",
    "                Val Loss: {logger['val_loss'][j]:.6f} \\\n",
    "                Val Acc: {logger['acc'][j]:.6f}\"\n",
    "            )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if config[\"save_model\"]:\n",
    "                # make sure folder is created to place saved checkpoints\n",
    "                path = Path.cwd() / \"models\" / net._name\n",
    "                if not path.exists():\n",
    "                    path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "                # pad with appropriate number of zeros i.e. epoch 10 named as 010\n",
    "                checkpoint_num = str(j + 1).zfill(len(str(config[\"num_epochs\"])))\n",
    "                model_path = f\"./models/{net._name}/{net._name}_{checkpoint_num}.pt\"\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print(f\"{config['num_epochs']} epochs took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    if config[\"log_training\"]:\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ShallowCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShallowCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, (7, 7), stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(96, 64, (5, 5), stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3, 3), stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(1152, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "net = ShallowCNN().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": net,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 3,\n",
    "    \"save_model\": False,\n",
    "    \"num_epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "test_data = STLData(mode=\"test\", transform=torchvision.transforms.ToTensor())\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_model(net, test_dataloader, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log(log, model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DeepCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "\n",
    "        self.blocks = self._build_blocks()\n",
    "        # TODO\n",
    "        # self.g_avgpool = nn.AvgPool2d()\n",
    "        self.fc1 = nn.Linear(192, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def _build_blocks(self):\n",
    "        conv_blk_dims = [3, 32, 64, 128, 192]\n",
    "        blocks_list = []\n",
    "        for i in range(len(conv_blk_dims) - 1):\n",
    "            conv_block = self._create_conv_block(conv_blk_dims[i], conv_blk_dims[i + 1])\n",
    "            blocks_list.append(conv_block)\n",
    "        return nn.Sequential(*blocks_list)\n",
    "\n",
    "    def _create_conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create conv_block based on in/out channels\"\"\"\n",
    "        conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (3, 3), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, (1, 1), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        return conv_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        # TODO\n",
    "        # x = self.g_avgpool(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = DeepCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
