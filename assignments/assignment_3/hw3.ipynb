{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Example\n",
    "\n",
    "the following class reads the data for the third assignment and creates a torch dataset object for it. With this, you can easily use a dataloader to train your model. \n",
    "\n",
    "Due to size limit on moodle, the data for this assignment should be obtained from \n",
    "\n",
    "https://drive.google.com/file/d/1khzPamThzWScipEfMmOPevtfWV7Tx6UL/view?usp=sharing\n",
    "\n",
    "\n",
    "Make sure that the file \"hw3.npz\" is located properly (in this example, it should be in the same folder as this notebook).\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STLData(Dataset):\n",
    "    def __init__(self, mode=\"\", transform=None):\n",
    "        data = np.load(\"hw3.npz\")\n",
    "        if \"train\" in mode:\n",
    "            # trainloader\n",
    "            self.images = data[\"arr_0\"]\n",
    "            self.labels = data[\"arr_1\"]\n",
    "        elif \"val\" in mode:\n",
    "            # valloader\n",
    "            self.images = data[\"arr_2\"]\n",
    "            self.labels = data[\"arr_3\"]\n",
    "        elif \"test\" in mode:\n",
    "            # testloader\n",
    "            self.images = data[\"arr_4\"]\n",
    "            self.labels = data[\"arr_5\"]\n",
    "        else:\n",
    "            raise ValueError(\"mode should be 'train', 'val' or 'test'\")\n",
    "\n",
    "        self.images = np.float32(self.images) / 1.0\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.images[idx, :]\n",
    "        labels = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how you can create a dataloader. \n",
    "First read the data. Note that the STL10 class can work with torchvision.transforms that are required in HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_set = STLData(trn_val_tst=0, transform=torchvision.transforms.ToTensor())\n",
    "# val_set = STLData(trn_val_tst=1, transform=torchvision.transforms.ToTensor())\n",
    "# test_set = STLData(trn_val_tst=2, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# modified STLData class\n",
    "train_set = STLData(\"train\", transform=torchvision.transforms.ToTensor())\n",
    "val_set = STLData(\"val\", transform=torchvision.transforms.ToTensor())\n",
    "test_set = STLData(\"test\", transform=torchvision.transforms.ToTensor())\n",
    "batch_size = 100\n",
    "n_workers = 0 * multiprocessing.cpu_count()\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=n_workers\n",
    ")\n",
    "image_batch, labels = next(iter(trainloader))\n",
    "\n",
    "fig, ax_arr = plt.subplots(2, 4)\n",
    "for i in range(8):\n",
    "    img = (image_batch[i] / 255.0).permute(1, 2, 0)\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    ax_arr[row, col].imshow(img)\n",
    "    # ax_arr[i // 4, i % 4].axis(\"off\")\n",
    "    ax_arr[row, col].axes.get_yaxis().set_visible(False)\n",
    "    ax_arr[row, col].set_xlabel(labels[i].item())\n",
    "    ax_arr[row, col].set_xticklabels([])\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a batchsize of 100, you can have a dataloader as follows for your training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = STLData(\"train\", transform=torchvision.transforms.ToTensor())\n",
    "val_set = STLData(\"val\", transform=torchvision.transforms.ToTensor())\n",
    "test_set = STLData(\"test\", transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_log(log, model_config, save=False, select=True):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figheight(7.5)\n",
    "    fig.set_figwidth(12)\n",
    "    # use ax1 for loss, ax2 for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    epochs = model_config.get(\"num_epochs\")\n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "    color = iter(cm.rainbow(np.linspace(0, 1, len(log))))\n",
    "    selected = dict.fromkeys(log)\n",
    "\n",
    "    count = 0\n",
    "    for key, values in log.items():\n",
    "        c = next(color)\n",
    "        # plot data\n",
    "        if \"loss\" in key:\n",
    "            ax1.plot(x_axis, values, color=c, label=key)\n",
    "        elif \"acc\" in key:\n",
    "            ax2.plot(x_axis, values, color=c, label=key)\n",
    "        if select:\n",
    "            if \"loss\" in key:\n",
    "                # search for min\n",
    "                x = np.argmin(values) + 1\n",
    "                y = np.amin(values)\n",
    "                ax1.plot(\n",
    "                    x,\n",
    "                    y,\n",
    "                    color=c,\n",
    "                    label=f\"Min. {key}\",\n",
    "                    markersize=16,\n",
    "                    marker=\"x\",\n",
    "                )\n",
    "            elif \"acc\" in key:\n",
    "                # search for max\n",
    "                x = np.argmax(log[key]) + 1\n",
    "                y = np.amax(log[key])\n",
    "                ax2.plot(\n",
    "                    x,\n",
    "                    y,\n",
    "                    color=c,\n",
    "                    label=f\"Max. {key}\",\n",
    "                    markersize=16,\n",
    "                    marker=\"x\",\n",
    "                )\n",
    "            # save values in dict\n",
    "            selected[key] = [x, y]\n",
    "        count += 1\n",
    "\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "    # 0 = 'best', 7 = 'center right'\n",
    "    fig.legend(loc=7, bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f\"./LR_{model_config['lr']}_{model_config['num_epochs']}.jpg\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if select:\n",
    "        return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(net, data_generator, loss_fn):\n",
    "    \"\"\"Function to easily test model on specified dataset\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_loss, batch_steps = 0.0, 0\n",
    "        correct_pred, total_pred = 0, 0\n",
    "\n",
    "        for batch_id, (data, label) in enumerate(data_generator):\n",
    "            data, label = data.to(device), label.long().to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            batch_loss += loss_fn(output, label).item()\n",
    "            batch_steps += 1\n",
    "\n",
    "            # indices where probability is maximum\n",
    "            _, pred_label = torch.max(output, 1)\n",
    "            correct_pred += (pred_label == label).sum().item()\n",
    "            total_pred += label.shape[0]\n",
    "\n",
    "        # average loss/acc across ALL batches\n",
    "        # i.e. ACROSS specified dataset\n",
    "        avg_loss = batch_loss / batch_steps\n",
    "        avg_acc = correct_pred / total_pred\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_workers = 0 * torch.cuda.device_count()\n",
    "\n",
    "    logger = {\n",
    "        \"train_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_loss\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"train_acc\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"val_acc\": np.zeros(config[\"num_epochs\"]),\n",
    "        \"test_acc\": np.zeros(config[\"num_epochs\"]),\n",
    "    }\n",
    "\n",
    "    #### LOAD DATA ####\n",
    "    b_size = config[\"batch_size\"]\n",
    "\n",
    "    train_data = STLData(mode=\"train\", transform=torchvision.transforms.ToTensor())\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_data = STLData(mode=\"val\", transform=torchvision.transforms.ToTensor())\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_data = STLData(mode=\"test\", transform=torchvision.transforms.ToTensor())\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=b_size,\n",
    "        num_workers=n_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    #### INSTANTIATE MODEL ####\n",
    "    net = config[\"model\"].to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if config[\"lr_scheduler\"]:\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \"max\", 0.5, patience=config[\"num_epochs\"] // 10, verbose=True\n",
    "        )\n",
    "\n",
    "    #### BEGIN TRAINING ####\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    for j in range(config[\"num_epochs\"]):\n",
    "        ## START OF EPOCH ##\n",
    "        train_loss, train_steps = 0.0, 0\n",
    "        net.train()\n",
    "        for batch_id, (data, label) in enumerate(train_dataloader):\n",
    "            data, label = data.to(device), label.long().to(device)\n",
    "\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = net(data)\n",
    "                loss = loss_function(output, label)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        ## END OF EPOCH ##\n",
    "\n",
    "        # average training loss for 1 epoch\n",
    "        train_loss /= train_steps\n",
    "\n",
    "        # test model on validation dataset\n",
    "        _, train_acc = test_model(net, train_dataloader, loss_function)\n",
    "        val_loss, val_acc = test_model(net, val_dataloader, loss_function)\n",
    "        _, test_acc = test_model(net, test_dataloader, loss_function)\n",
    "\n",
    "        if config[\"lr_scheduler\"]:\n",
    "            scheduler.step(val_acc)\n",
    "\n",
    "        logger[\"train_loss\"][j] = train_loss\n",
    "        logger[\"val_loss\"][j] = val_loss\n",
    "        logger[\"train_acc\"][j] = train_acc\n",
    "        logger[\"val_acc\"][j] = val_acc\n",
    "        logger[\"test_acc\"][j] = test_acc\n",
    "\n",
    "        if config[\"log_training\"] and (j + 1) % config[\"log_interval\"] == 0:\n",
    "            print(\n",
    "                f\"Epoch:{j+1}/{config['num_epochs']}\",\n",
    "                f\"Train Loss: {logger['train_loss'][j]:.4f}\",\n",
    "                f\"Train Acc: {logger['train_acc'][j]:.4f}\",\n",
    "                f\"Val Loss: {logger['val_loss'][j]:.4f}\",\n",
    "                f\"Val Acc: {logger['val_acc'][j]:.4f}\",\n",
    "                f\"Test Acc: {logger['test_acc'][j]:.4f}\",\n",
    "            )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if config[\"save_model\"]:\n",
    "                # make sure folder is created to place saved checkpoints\n",
    "                path = Path.cwd() / \"models\" / net._name\n",
    "                if not path.exists():\n",
    "                    path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "                # pad with appropriate number of zeros i.e. epoch 10 named as 010\n",
    "                checkpoint_num = str(j + 1).zfill(len(str(config[\"num_epochs\"])))\n",
    "                model_path = f\"./models/{net._name}/{net._name}_{checkpoint_num}.pt\"\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print(f\"{config['num_epochs']} epochs took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    if config[\"log_training\"]:\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ShallowCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShallowCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, (7, 7), stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(96, 64, (5, 5), stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3, 3), stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(1152, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shallow_net = ShallowCNN().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": shallow_net,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_scheduler\": False,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "    \"num_epochs\": 60,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_shallow = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log(log_shallow, model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_net = ShallowCNN().to(device)\n",
    "shallow_net.eval()\n",
    "# select 210, 240, 1490 models\n",
    "model_path = f\"models/{shallow_net._name}/{shallow_net._name}_20.pt\"\n",
    "shallow_net.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data = STLData(mode=\"val\", transform=torchvision.transforms.ToTensor())\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3D list, where index = class label\n",
    "# each row of top level list, format: [prediction prob, True or False, image data]\n",
    "correct_bin = [[] for _ in range(10)]\n",
    "wrong_bin = [[] for _ in range(10)]\n",
    "with torch.no_grad():\n",
    "    shallow_net.eval()\n",
    "    # loop through val dataset, collect all scores per class\n",
    "    for batch_id, (data, label) in enumerate(val_dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = shallow_net(data)\n",
    "        # apply softmax\n",
    "        probs = F.softmax(output, dim=1)\n",
    "\n",
    "        # output True/False for each batch\n",
    "        all_idx = torch.argmax(output, 1) == label\n",
    "        for idx, val in enumerate(all_idx):\n",
    "            label_ = label[idx]\n",
    "            # store in correct_bin\n",
    "            if val == True:\n",
    "                correct_bin[label_].append([probs[idx].max(), all_idx[idx], data[idx]])\n",
    "            # store in wrong_bin\n",
    "            elif val == False:\n",
    "                wrong_bin[label_].append([probs[idx].max(), all_idx[idx], data[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_bin(data_bin):\n",
    "    num_img = 5\n",
    "    all_imgs = torch.empty(len(data_bin) * num_img, 3, 96, 96)\n",
    "    for i, list_2d in enumerate(data_bin):\n",
    "        # sort based on first element i.e. probabilities\n",
    "        top_5 = sorted(list_2d, key=lambda x: x[0], reverse=True)[:num_img]\n",
    "        top_5_tensor = torch.empty(num_img, *top_5[0][2].shape)\n",
    "        for j in range(num_img):\n",
    "            top_5_tensor[j] = top_5[j][2]\n",
    "        offset = i * num_img\n",
    "        all_imgs[0 + offset : 5 + offset] = top_5_tensor\n",
    "\n",
    "    all_imgs = all_imgs / 255.0\n",
    "    out = torchvision.utils.make_grid(all_imgs, nrow=num_img)\n",
    "    fig, ax = plt.subplots(figsize=(10, 20))\n",
    "    ax.imshow(out.permute(1, 2, 0), interpolation=\"nearest\", aspect=\"auto\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_bin(correct_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_bin(wrong_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # loop through different classes\n",
    "# fig, ax = plt.subplots(10, 5)\n",
    "# for i, list_2d in enumerate(correct_bin):\n",
    "#     # sort based on first element i.e. probabilities\n",
    "#     top_5 = sorted(list_2d, key=lambda x: x[0], reverse=True)[:5]\n",
    "#     for j in range(5):\n",
    "#         img = (top_5[j][2] / 255.0).permute(1, 2, 0).cpu()\n",
    "#         ax[i, j].imshow(img)\n",
    "#         ax[i, j].axes.get_yaxis().set_visible(False)\n",
    "#         ax[i, j].axes.get_xaxis().set_visible(False)\n",
    "# fig.set_figheight(10)\n",
    "# fig.set_figwidth(10)\n",
    "# plt.subplots_adjust(wspace=0.00, hspace=0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# DeepCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "\n",
    "        self.blocks = self._build_blocks()\n",
    "        # since the output of our conv blocks is (6,6)\n",
    "        self.gap = nn.AvgPool2d(kernel_size=6, stride=1)\n",
    "        self.fc1 = nn.Linear(192, 10)\n",
    "\n",
    "        self._name = self.__class__.__name__\n",
    "\n",
    "    def _build_blocks(self):\n",
    "        conv_blk_dims = [3, 32, 64, 128, 192]\n",
    "        blocks_list = []\n",
    "        for i in range(len(conv_blk_dims) - 1):\n",
    "            conv_block = self._create_conv_block(conv_blk_dims[i], conv_blk_dims[i + 1])\n",
    "            named_block = (f\"Conv-Blk-{i+1}\", conv_block)\n",
    "            # blocks_list.append(conv_block)\n",
    "            blocks_list.append(named_block)\n",
    "\n",
    "        # return nn.Sequential(*blocks_list)\n",
    "        return nn.Sequential(OrderedDict(blocks_list))\n",
    "\n",
    "    def _create_conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create conv_block based on in/out channels\"\"\"\n",
    "        conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (3, 3), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, (1, 1), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        return conv_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze()\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = DeepCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_net = DeepCNN().to(device)\n",
    "model_cfg = {\n",
    "    \"model\": deep_net,\n",
    "    \"lr\": 2.5e-4,\n",
    "    \"lr_scheduler\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"log_training\": True,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"num_epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log = train_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log(log, model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "test_dataloader = DataLoader(\n",
    "    STLData(mode=\"test\", transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=model_cfg[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_model(deep_net, test_dataloader, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
