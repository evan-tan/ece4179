{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE4179 - Assignment\\#1 \n",
    "## Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT WHEN SUBMITTING\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the sigmoid function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # this function should compute the sigmoid of x\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the sigmoid function and write a predictor for the logistic model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    this function should get X, an array of samples, and theta, the parameters\n",
    "    of the logistic model and generate 0 or 1 as the label of each sample in X\n",
    "    #the rule is that, if the sigmoid of x >= 0.5, we predict the label of x to be 1\n",
    "    otherwise the label is 0\n",
    "    \"\"\"\n",
    "    return ((X @ theta.T) >= 0.5).astype(int) # jank boolean conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function to compute the loss and gradient for the logistic model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-100  # arbitrarily small number\n",
    "\n",
    "\n",
    "def compute_grad_loss(X, y, theta):\n",
    "    # this function will get X, a set of samples (each sample is a row in X),\n",
    "    # the corresponding labels in the array y and the current parameter of the logistic model theta\n",
    "    # use the sigmoid function to compute the loss and the gradient of samples with respect to theta\n",
    "    # when computing the loss value, pay extra attention to the log function. log(0) can cause problems so you need\n",
    "    # to handle it\n",
    "    wx = X @ theta.T\n",
    "    sig_wx = sigmoid(wx)\n",
    "    sig_wx = np.where(sig_wx == 0, epsilon, sig_wx)\n",
    "\n",
    "    loss = -np.mean(y.T @ np.log(sig_wx) + (1 - y).T @ np.log(1 - sig_wx))\n",
    "    # if theta is (N,1), use axis = 1\n",
    "    grad_vec = np.mean((sig_wx - y).T @ X, axis=0)\n",
    "\n",
    "    return loss, grad_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use this cell to load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load your data using this cell\n",
    "\n",
    "npzfile = np.load(\"toy_data.npz\")  # toy_data.npz or toy_data_two_circles.npz\n",
    "\n",
    "\n",
    "X_train = npzfile[\"arr_0\"]\n",
    "X_test = npzfile[\"arr_1\"]\n",
    "y_train = npzfile[\"arr_2\"]\n",
    "y_test = npzfile[\"arr_3\"]\n",
    "\n",
    "\n",
    "# remember that each row in X_train and X_test is a sample. so X_train[1,:] is the first training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### you can plot the data using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will plot the data\n",
    "plt.subplot(121)\n",
    "plt.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], marker=\"o\", c=y_train[:, 0], s=25, edgecolor=\"k\"\n",
    ")\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker=\"o\", c=y_test[:, 0], s=25, edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below you need to implement the gradient descent (GD) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta is the parameters of the logistic model\n",
    "np.random.seed(0)\n",
    "# To learn them, we randomly initilize them below\n",
    "# theta = np.random.randn(X_train.shape[1], 1)\n",
    "theta = np.random.randn(1, X_train.shape[1])  # shape (1,2)\n",
    "\n",
    "# this is the learning rate of the GD algorithm, you need to tune this and study its effects in your report\n",
    "lr = 5e-4\n",
    "\n",
    "# this is the maximum number of iterations of the GD algorithm.\n",
    "# Since we use the GD, each iteration of the algorithm is equivalent to one epoch, hence the name\n",
    "max_epoch = 500\n",
    "\n",
    "# keep track of the loss/accuracy values for plotting\n",
    "loss = np.zeros(max_epoch)\n",
    "accuracy = np.zeros(max_epoch)\n",
    "for epoch in range(max_epoch):\n",
    "    # call the compute_grad_loss that you have implemented above to\n",
    "    # measure the loss and the gradient\n",
    "    loss[epoch], grad_vec = compute_grad_loss(X_train, y_train, theta)\n",
    "    # update the theta parameter according to the GD here\n",
    "    theta -= lr * grad_vec\n",
    "\n",
    "    # storage for plotting\n",
    "    y_test_hat = predict(X_test, theta)\n",
    "    accuracy[epoch] = float(sum(y_test_hat == y_test)) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot our training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(accuracy, \"r-\")\n",
    "ax2.plot(loss, \"g-\")\n",
    "\n",
    "ax1.set_xlabel(\"Number of epochs\")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"r\")\n",
    "ax2.set_ylabel(\"Loss\", color=\"g\")\n",
    "plt.title(\"Test Accuracy and Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your trained model using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the predictions are either 0 or 1 and the shape of y_test_hat\n",
    "(y_test_hat >= 0).all() and (y_test_hat <= 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that shapes match\n",
    "y_test_hat.shape == y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that you have trained your model, let's evaluate it\n",
    "\n",
    "# first call the predict function on your test data with the parameters obtained by GD\n",
    "y_test_hat = predict(X_test, theta)\n",
    "\n",
    "# the script below, if the dimensionality of the arrays is set correctly,\n",
    "# will measure how many samples are correctly classified by your model\n",
    "score = float(sum(y_test_hat == y_test)) / float(len(y_test))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
