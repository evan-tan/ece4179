{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE4179 - Assignment\\#1 \n",
    "## Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the sigmoid function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # this function should compute the sigmoid of x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the sigmoid function and write a predictor for the logistic model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    this function should get X, an array of samples, and theta, the parameters\n",
    "    of the logistic model and generate 0 or 1 as the label of each sample in X\n",
    "    #the rule is that, if the sigmoid of x >= 0.5, we predict the label of x to be 1\n",
    "    otherwise the label is 0\n",
    "    \"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function to compute the loss and gradient for the logistic model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_loss(X, y, theta):\n",
    "    #this function will get X, a set of samples (each sample is a row in X),\n",
    "    #the corresponding labels in the array y and the current parameter of the logistic model theta\n",
    "    \n",
    "    #use the sigmoid function to compute the loss and the gradient of samples with respect to theta\n",
    "  \n",
    "    #when computing the loss value, pay extra attention to the log function. log(0) can cause problems so you need\n",
    "    #to handle it\n",
    "    \n",
    "    return loss, grad_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use this cell to load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load your data using this cell\n",
    "\n",
    "npzfile = np.load(\"toy_data.npz\") # toy_data.npz or toy_data_two_circles.npz\n",
    "\n",
    "\n",
    "X_train = npzfile['arr_0']\n",
    "X_test = npzfile['arr_1']\n",
    "y_train = npzfile['arr_2']\n",
    "y_test = npzfile['arr_3']\n",
    "\n",
    "\n",
    "# remember that each row in X_train and X_test is a sample. so X_train[1,:] is the first training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### you can plot the data using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will plot the data\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train[:,0], s=25, edgecolor='k')\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test[:,0], s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below you need to implement the gradient descent (GD) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta is the parameters of the logistic model\n",
    "\n",
    "# To learn them, we randomly initilize them below\n",
    "theta = np.random.randn(X_train.shape[1],1)\n",
    "\n",
    "\n",
    "#this is the learning rate of the GD algorithm, you need to tune this and study its effects in your report\n",
    "lr = 0 \n",
    "\n",
    "# this is the maximum number of iterations of the GD algorithm. \n",
    "# Since we use the GD, each iteration of the algorithm is equivalent to one epoch, hence the name\n",
    "max_epoch = 0 \n",
    "\n",
    "\n",
    "loss = np.zeros(max_epoch) #keep track of the loss values for plotting\n",
    "for epoch in range(max_epoch):\n",
    "    # call the compute_grad_loss that you have implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    loss[epoch], grad_vec = compute_grad_loss(X_train, y_train, theta)\n",
    "    #update the theta parameter according to the GD here\n",
    "    theta =  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your trained model using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that you have trained your model, let's evaluate it \n",
    "\n",
    "# first call teh predict function on your test data with the parameters obtained by DG\n",
    "y_test_hat = predict(X_test,theta)\n",
    "# make sure that the predictions are either 0 or 1 and the shape of y_test_hat\n",
    "# matches that ofy_test\n",
    "\n",
    "# the script below, if the dimensionality of the arrays is set correctly,\n",
    "# will measure how many samples are correctly classified by your model\n",
    "score = float(sum(y_test_hat == y_test))/ float(len(y_test))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
